//
//  ConversationController.swift
//

import Foundation
import AVFoundation
import Speech
import Domain
import Services
import Support
import DataStores
import Network

@MainActor
public final class ConversationController: NSObject, ObservableObject {

    // MARK: - UI State
    public enum Mode: String, CaseIterable { case localSTT, realtime }

    // âœ… æ©Ÿèƒ½ãƒˆã‚°ãƒ«
    public static let localSTTEnabled: Bool = true
    public var isLocalSTTEnabled: Bool { Self.localSTTEnabled }

    @Published public var mode: Mode = .localSTT
    @Published public var transcript: String = ""
    @Published public var isRecording: Bool = false
    @Published public var errorMessage: String?
    @Published public var isRealtimeActive: Bool = false
    @Published public var isRealtimeConnecting: Bool = false
    // âœ… éŸ³å£°ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã®ãƒ­ãƒ¼ã‚«ãƒ«æ–‡å­—èµ·ã“ã—ã‚’è¡Œã†ã‹ï¼ˆç«¯æœ«ç’°å¢ƒã§ kAFAssistantErrorDomain 1101 ãŒå¤šç™ºã™ã‚‹ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆOFFï¼‰
    private let enableLocalUserTranscription: Bool = ConversationController.localSTTEnabled
    
    // è¿½åŠ : ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒåœæ­¢ã—ãŸã‹ã‚’è¦šãˆã‚‹ãƒ•ãƒ©ã‚°
    private var userStoppedRecording = false
    
    // âœ… AIéŸ³å£°å†ç”Ÿä¸­ãƒ•ãƒ©ã‚°ï¼ˆonAudioDeltaReceivedã§è¨­å®šã€sendMicrophonePCMã®æ—©æœŸreturnã‚’ä¸€å…ƒåŒ–ï¼‰
    @Published private(set) var isAIPlayingAudio: Bool = false
    // âœ… ãƒãƒ³ã‚ºãƒ•ãƒªãƒ¼ãƒ¢ãƒ¼ãƒ‰ã®æœ‰åŠ¹åŒ–ãƒ•ãƒ©ã‚°
    @Published public var isHandsFreeMode: Bool = false
    
    // âœ… ã‚¿ãƒ¼ãƒ³çŠ¶æ…‹ï¼ˆæ‹¡å¼µç‰ˆï¼‰
    enum TurnState: Equatable {
        case idle               // ã‚»ãƒƒã‚·ãƒ§ãƒ³å‰ or çµ‚äº†å¾Œ
        case waitingUser        // åˆå›/æ¯ã‚¿ãƒ¼ãƒ³ï¼šã¾ãšãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å£°ã‚’å¾…ã¤
        case nudgedByAI(Int)   // ä¿ƒã—(ä½•å›ç›®ã‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹)
        case listening          // VADãŒ speech_started ã€œ speech_stopped ã®é–“
        case thinking           // commit æ¸ˆã¿ã€œå¿œç­”ç”Ÿæˆä¸­
        case speaking           // AIãŒTTSå‡ºåŠ›ä¸­
        case clarifying         // èãå–ã‚Šä¸å¯â†’èãè¿”ã—ä¸­
    }
    @Published private(set) var turnState: TurnState = .idle
    
    // âœ… ã€Œå¾…ã¤â†’ä¿ƒã™ã€ã‚¿ã‚¤ãƒãƒ¼
    private var nudgeTimer: Timer?
    // âœ… ä¿ƒã—å›æ•°ã®ä¸Šé™ã¨ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
    private let maxNudgeCount = AppConfig.nudgeMaxCount
    private var nudgeCount = 0
    
    // âœ… è¿½åŠ : æœ€å¾Œã«ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å£°ï¼ˆç’°å¢ƒéŸ³å«ã‚€ï¼‰ã€ãŒé–¾å€¤ã‚’è¶…ãˆãŸæ™‚åˆ»
    private var lastUserVoiceActivityTime: Date = Date()
    private var lastInputRMS: Double?
    
    // âœ… è¿½åŠ : ç„¡éŸ³åˆ¤å®šã®é–¾å€¤ï¼ˆ-50dBã‚ˆã‚Šå¤§ãã‘ã‚Œã°ã€Œä½•ã‹éŸ³ãŒã—ã¦ã„ã‚‹ã€ã¨ã¿ãªã™ï¼‰
    // èª¿æ•´ç›®å®‰: -40dB(æ™®é€š) ã€œ -60dB(é™å¯‚)ã€‚-50dBã¯ã€Œã•ã•ã‚„ãå£°ã‚„ç’°å¢ƒéŸ³ã€ãƒ¬ãƒ™ãƒ«
    private let silenceThresholdDb: Double = -50.0
    
    // âœ… è¿½åŠ : speech_startedãŒæ¥ã¦ã„ãªã„è­¦å‘Šã®ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
    private var speechStartedMissingCount: Int = 0
    
    // MARK: - VAD (Hands-free Conversation)
    enum VADState { case idle, speaking }
    // ğŸ”§ Temporarily extremely low thresholds to force VAD triggering
    // ğŸ”§ Loosened thresholds to verify sensitivity (see VAD logging)
    private let speechStartThreshold: Float = 0.005
    private let speechEndThreshold: Float = 0.002
    private let defaultRmsStartThresholdDb: Double = -35.0
    private let bluetoothRmsStartThresholdDb: Double = -45.0
    private let defaultSpeechEndRmsThresholdDb: Double = -45.0
    private let bluetoothSpeechEndRmsThresholdDb: Double = -42.0
    private let defaultMinSilenceDuration: TimeInterval = 1.0
    private let bluetoothMinSilenceDuration: TimeInterval = 1.0
    private let speechStartHoldDuration: TimeInterval = 0.15
    private let minSpeechDuration: TimeInterval = 0.25
    // âœ… å‰²ã‚Šè¾¼ã¿åˆ¤å®šç”¨ï¼ˆSilero VADã‚’ä¸€å®šæ™‚é–“é€£ç¶šæ¤œå‡ºã—ãŸå ´åˆã®ã¿åœæ­¢ï¼‰
    private let bargeInVADThreshold: Float = 0.5
    private let bargeInHoldDuration: TimeInterval = 0.15  // 150ms ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹
    private var vadInterruptSpeechStart: Date?
    @Published private(set) var vadState: VADState = .idle
    private var speechStartTime: Date?
    private var silenceTimer: Timer?
    private var isUserSpeaking: Bool = false
    private var speechStartCandidateTime: Date?
    
    private var isBluetoothInput: Bool {
        let session = AVAudioSession.sharedInstance()
        return session.currentRoute.inputs.contains { $0.portType == .bluetoothHFP }
    }
    private var activeRmsStartThresholdDb: Double {
        isBluetoothInput ? bluetoothRmsStartThresholdDb : defaultRmsStartThresholdDb
    }
    private var activeSpeechEndRmsThresholdDb: Double {
        isBluetoothInput ? bluetoothSpeechEndRmsThresholdDb : defaultSpeechEndRmsThresholdDb
    }
    private var activeMinSilenceDuration: TimeInterval {
        isBluetoothInput ? bluetoothMinSilenceDuration : defaultMinSilenceDuration
    }
    
    // âœ… å„ã‚¿ãƒ¼ãƒ³ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¨ˆæ¸¬ç”¨
    private struct TurnMetrics {
        var listenStart: Date?
        var speechEnd: Date?
        var requestStart: Date?
        var firstByte: Date?
        var firstAudio: Date?
        var firstText: Date?
        var streamComplete: Date?
        var playbackEnd: Date?
    }
    private var turnMetrics = TurnMetrics()
    
    // ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
    @Published public var aiResponseText: String = ""
    @Published public var isPlayingAudio: Bool = false
    @Published public var hasMicrophonePermission: Bool = false
    @Published public var liveSummary: String = ""                 // ä¼šè©±ã®ç°¡æ˜“è¦ç´„ï¼ˆæ¯ã‚¿ãƒ¼ãƒ³æ›´æ–°ï¼‰
    @Published public var liveInterests: [FirebaseInterestTag] = [] // ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«æ›´æ–°
    @Published public var liveNewVocabulary: [String] = []          // ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«æ›´æ–°
    // âœ… å‰²ã‚Šè¾¼ã¿å¾Œã«æµå…¥ã™ã‚‹å¤ã„AIãƒãƒ£ãƒ³ã‚¯ã‚’ç„¡è¦–ã™ã‚‹ãŸã‚ã®ã‚²ãƒ¼ãƒˆ
    private var ignoreIncomingAIChunks: Bool = false
    private var currentTurnId: Int = 0         // ã‚¿ãƒ¼ãƒ³ã®ä¸–ä»£IDï¼ˆå˜ä¸€ã®çœŸå®Ÿï¼‰
    private var listeningTurnId: Int = 0       // VAD/éŒ²éŸ³ç”¨ã®ä¸–ä»£ID
    private var playbackTurnId: Int?           // å†ç”ŸçŠ¶æ…‹é€šçŸ¥ã®ä¸–ä»£ID
    
    // AIå‘¼ã³å‡ºã—ç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    @Published public var isThinking: Bool = false   // ãã‚‹ãã‚‹è¡¨ç¤ºç”¨
    private var lastAskedText: String = ""           // åŒæ–‡ã®é€£æŠ•é˜²æ­¢

    // MARK: - Local STT (Speech) - DIå¯¾å¿œ
    private let audioEngine = AVAudioEngine()
    private let audioSession: AudioSessionManaging
    private let speech: SpeechRecognizing
    private var sttRequest: SFSpeechAudioBufferRecognitionRequest?
    private var sttTask: SpeechRecognitionTasking?

    // MARK: - Realtime (OpenAI)
    private let audioSessionManager = AudioSessionManager()
    // âœ… AECæœ‰åŠ¹åŒ–ã®ãŸã‚ã€å…±é€šã®AVAudioEngineã‚’ä½¿ç”¨
    private let sharedAudioEngine = AVAudioEngine()
    private var mic: MicrophoneCapture?
    private var player: PlayerNodeStreamer            // éŸ³å£°å…ˆå‡ºã—ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    // âœ… Realtime API ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ã€gpt-4o-audio-preview ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã«åˆ‡ã‚Šæ›¿ãˆ
    private var realtimeClient: RealtimeClientOpenAI?
    private var audioPreviewClient: AudioPreviewStreamingClient?
    private var recordedPCMData = Data()
    private var recordedSampleRate: Double = 24_000
    // âœ… ç›¸æ§Œå†ç”ŸON/OFFï¼ˆå½“é¢OFFã«ã™ã‚‹ï¼‰
    private let enableFillers = true
    // âœ… å†ç”Ÿã™ã‚‹ç›¸æ§Œãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆãƒãƒ³ãƒ‰ãƒ«ã«è¿½åŠ ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åï¼‰
    private let fillerFiles = [
        "ã†ã‚“ã†ã‚“", "ãã£ã‹", "ãµãƒ¼ã‚“", "ã¸ãƒ¼"
    ]
    // âœ… ç›¸æ§Œå†ç”Ÿä¸­ã‹ã®ãƒ•ãƒ©ã‚°ï¼ˆAIéŸ³å£°ãŒæ¥ãŸã‚‰ä¸€åº¦ã ã‘æ­¢ã‚ã‚‹ï¼‰
    private var isFillerPlaying = false
    private var receiveTextTask: Task<Void, Never>?
    private var receiveAudioTask: Task<Void, Never>?
    private var receiveInputTextTask: Task<Void, Never>?
    private var sessionStartTask: Task<Void, Never>?     // ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹ã‚¿ã‚¹ã‚¯ã®ç®¡ç†
    private var liveSummaryTask: Task<Void, Never>?      // ãƒ©ã‚¤ãƒ–è¦ç´„ç”Ÿæˆã‚¿ã‚¹ã‚¯
    private var inMemoryTurns: [FirebaseTurn] = []       // ä¼šè©±ãƒ­ã‚°ï¼ˆè¦ç´„ç”¨ï¼‰
    private var routeChangeObserver: Any?
    
    // âœ… ä¼šè©±æ–‡è„ˆï¼ˆéå»ã®ãƒ†ã‚­ã‚¹ãƒˆå±¥æ­´ï¼‰ã‚’ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹APIã«æ¸¡ã™ãŸã‚ã«ä¿æŒ
    struct HistoryItem: Codable {
        let role: String    // "user" or "assistant"
        let text: String
    }
    private var conversationHistory: [HistoryItem] = []
    
    // MARK: - Firebaseä¿å­˜
    private let firebaseRepository = FirebaseConversationsRepository()
    private var currentSessionId: String?
    // âœ… èªè¨¼æƒ…å ±ï¼ˆAuthViewModelã‹ã‚‰è¨­å®šã•ã‚Œã‚‹ï¼‰
    private var currentUserId: String?
    private var currentChildId: String?
    private var currentChildName: String?
    private var currentChildNickname: String?
    private var turnCount: Int = 0
    
    // ä¼šè©±ã§å‘¼ã¶åå‰ï¼ˆãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ å„ªå…ˆï¼‰
    private var childCallName: String? {
        if let nickname = currentChildNickname?.trimmingCharacters(in: .whitespacesAndNewlines), !nickname.isEmpty {
            return nickname
        }
        if let name = currentChildName?.trimmingCharacters(in: .whitespacesAndNewlines), !name.isEmpty {
            return name
        }
        return nil
    }
    
    // âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’è¨­å®šã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰
    public func setupUser(userId: String, childId: String, childName: String? = nil, childNickname: String? = nil) {
        self.currentUserId = userId
        self.currentChildId = childId
        let trimmedName = childName?.trimmingCharacters(in: .whitespacesAndNewlines)
        let trimmedNickname = childNickname?.trimmingCharacters(in: .whitespacesAndNewlines)
        self.currentChildName = trimmedName?.isEmpty == true ? nil : trimmedName
        self.currentChildNickname = trimmedNickname?.isEmpty == true ? nil : trimmedNickname
        print("âœ… ConversationController: ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’è¨­å®š - Parent=\(userId), Child=\(childId), Name=\(childCallName ?? "n/a")")
    }
    
    private static func pcmFromWavIfPossible(_ data: Data) -> Data? {
        // RIFF/WAVEãƒ˜ãƒƒãƒ€ãƒã‚§ãƒƒã‚¯
        guard data.count >= 12,
              String(data: data[0..<4], encoding: .ascii) == "RIFF",
              String(data: data[8..<12], encoding: .ascii) == "WAVE" else {
            return nil
        }
        
        var offset = 12 // RIFFãƒ˜ãƒƒãƒ€ã®å¾Œã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’èµ°æŸ»
        var fmtFound = false
        var audioFormat: UInt16 = 0
        var bitsPerSample: UInt16 = 0
        
        while offset + 8 <= data.count {
            let chunkIDData = data[offset..<offset+4]
            let chunkSize = data[offset+4..<offset+8].withUnsafeBytes { $0.load(as: UInt32.self) }
            let chunkID = String(data: chunkIDData, encoding: .ascii) ?? ""
            let chunkStart = offset + 8
            let chunkEnd = chunkStart + Int(chunkSize)
            guard chunkEnd <= data.count else { return nil }
            
            if chunkID == "fmt " {
                // PCM16ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆç¢ºèª
                guard chunkSize >= 16 else { return nil }
                audioFormat = data[chunkStart..<chunkStart+2].withUnsafeBytes { $0.load(as: UInt16.self) }
                bitsPerSample = data[chunkStart+14..<chunkStart+16].withUnsafeBytes { $0.load(as: UInt16.self) }
                fmtFound = true
            } else if chunkID == "data" {
                guard fmtFound, audioFormat == 1, bitsPerSample == 16 else { return nil }
                let pcmRange = chunkStart..<chunkEnd
                return data.subdata(in: pcmRange)
            }
            
            // ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¯å¶æ•°å¢ƒç•Œã«æƒã†ãŸã‚ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°è€ƒæ…®
            offset = chunkEnd + (chunkSize % 2 == 1 ? 1 : 0)
        }
        return nil
    }

    /// Firebaseã‚¨ãƒ©ãƒ¼ã®è©³ç´°ãƒ­ã‚°å‡ºåŠ›ï¼ˆPermission deniedã®å ´åˆã«ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ«ãƒ¼ãƒ«ã®è¨­å®šæ–¹æ³•ã‚’æ¡ˆå†…ï¼‰
    private func logFirebaseError(_ error: Error, operation: String) {
        let errorString = String(describing: error)
        print("âŒ ConversationController: \(operation)å¤±æ•— - \(errorString)")
        
        // Permission deniedã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ«ãƒ¼ãƒ«ã®è¨­å®šæ–¹æ³•ã‚’æ¡ˆå†…
        if errorString.contains("Permission denied") || errorString.contains("Missing or insufficient permissions") {
            print("""
            âš ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ«ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚
            é–‹ç™ºç’°å¢ƒã§ã¯ã€Firebaseã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’è¨­å®šã—ã¦ãã ã•ã„:
            
            rules_version = '2';
            service cloud.firestore {
              match /databases/{database}/documents {
                match /{document=**} {
                  allow read, write: if true;
                }
              }
            }
            
            è©³ç´°ã¯ FIREBASE_SUMMARY.md ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
            """)
        }
    }

    // MARK: - Lifecycle
    public init(
        audioSession: AudioSessionManaging = SystemAudioSessionManager(),
        speech: SpeechRecognizing = SystemSpeechRecognizer(locale: "ja-JP")
    ) {
        self.audioSession = audioSession
        self.speech = speech
        // âœ… å…±é€šã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã—ã¦PlayerNodeStreamerã‚’åˆæœŸåŒ–ï¼ˆAECæœ‰åŠ¹åŒ–ã®ãŸã‚ï¼‰
        self.player = PlayerNodeStreamer(sharedEngine: sharedAudioEngine)
        super.init()
    }

    deinit {
        // âœ… deinitã¯åŒæœŸçš„ã«å®Ÿè¡Œã•ã‚Œã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€éåŒæœŸå‡¦ç†ã¯è¡Œã‚ãªã„
        // åŒæœŸçš„ã«å®Ÿè¡Œå¯èƒ½ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®ã¿ã‚’è¡Œã†
        
        // ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
        sessionStartTask?.cancel()
        sessionStartTask = nil
        if let observer = routeChangeObserver {
            NotificationCenter.default.removeObserver(observer)
            routeChangeObserver = nil
        }
        liveSummaryTask?.cancel()
        liveSummaryTask = nil
        
        // å—ä¿¡ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
        receiveTextTask?.cancel()
        receiveTextTask = nil
        receiveAudioTask?.cancel()
        receiveAudioTask = nil
        receiveInputTextTask?.cancel()
        receiveInputTextTask = nil
        
        // ãƒã‚¤ã‚¯ã¨ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åœæ­¢ï¼ˆdeinitã¯éisolatedãªã®ã§ç›´æ¥åœæ­¢ï¼‰
        mic?.stop()
        mic = nil
        player.stop()
        
        // å…±é€šã‚¨ãƒ³ã‚¸ãƒ³ã‚’åœæ­¢
        if sharedAudioEngine.isRunning {
            sharedAudioEngine.stop()
        }
        
        // ä¿ƒã—ã‚¿ã‚¤ãƒãƒ¼ã‚’åœæ­¢ï¼ˆdeinitå†…ã§ã¯ç›´æ¥ç„¡åŠ¹åŒ–ï¼‰
        // âœ… cancelNudge()ã¯@MainActorã§åˆ†é›¢ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€deinitå†…ã§ã¯ç›´æ¥ã‚¿ã‚¤ãƒãƒ¼ã‚’ç„¡åŠ¹åŒ–
        nudgeTimer?.invalidate()
        nudgeTimer = nil
        
        // Local STTã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        // âœ… removeTapã¯ã‚¿ãƒƒãƒ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã§ã‚‚ã‚¨ãƒ©ãƒ¼ã‚’æŠ•ã’ãªã„ãŸã‚ã€å®‰å…¨ã«å‘¼ã³å‡ºã›ã‚‹
        audioEngine.inputNode.removeTap(onBus: 0)
        if audioEngine.isRunning {
            audioEngine.stop()
        }
        sttRequest?.endAudio()
        sttTask?.cancel()
        sttRequest = nil
        sttTask = nil
        
        // realtimeClientã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆéåŒæœŸå‡¦ç†ã¯å®Ÿè¡Œã—ãªã„ï¼‰
        // finishSession()ã¯éåŒæœŸå‡¦ç†ã®ãŸã‚ã€deinitå†…ã§ã¯å®Ÿè¡Œã—ãªã„
        // ä»£ã‚ã‚Šã«ã€realtimeClientã®å‚ç…§ã‚’nilã«ã—ã¦ã€deinitæ™‚ã«è‡ªå‹•çš„ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
        realtimeClient = nil
        if let observer = routeChangeObserver {
            NotificationCenter.default.removeObserver(observer)
            routeChangeObserver = nil
        }
        
        print("âœ… ConversationController: deinit - ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
    }

    // MARK: - Permissions
    public func requestPermissions() {
        AVAudioSession.sharedInstance().requestRecordPermission { [weak self] granted in
            DispatchQueue.main.async {
                self?.hasMicrophonePermission = granted
            }
        }
        SFSpeechRecognizer.requestAuthorization { _ in }
    }

    // MARK: - Local STT (DIå¯¾å¿œç‰ˆ)
    public func startLocalTranscription() {
        guard !isRecording else { return }
        guard speech.isAvailable else {
            self.errorMessage = "éŸ³å£°èªè­˜ãŒç¾åœ¨åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚"
            return
        }
        if isHandsFreeMode {
            stopHandsFreeConversation()
        }

        // 1) AudioSession ã‚’å…ˆã«æ§‹æˆï¼ˆDIçµŒç”±ï¼‰
        do { try audioSession.configure() }
        catch {
            self.errorMessage = "AudioSessioné–‹å§‹ã«å¤±æ•—: \(error.localizedDescription)"
            return
        }

        // 2) ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
        let req = SFSpeechAudioBufferRecognitionRequest()
        req.shouldReportPartialResults = true
        self.sttRequest = req
        self.transcript = ""
        self.errorMessage = nil

        // 3) ãƒã‚¤ã‚¯ã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¸æµã™ï¼ˆformat: nil ã§è£…ç½®ã®æ­£ã—ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«è¿½éšï¼‰
        let input = audioEngine.inputNode
        input.removeTap(onBus: 0)
        input.installTap(onBus: 0, bufferSize: 1024, format: nil) { [weak self] buffer, _ in
            self?.sttRequest?.append(buffer)
        }

        // 4) ã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•
        do {
            audioEngine.prepare()
            try audioEngine.start()
        } catch {
            self.errorMessage = "AudioEngineé–‹å§‹ã«å¤±æ•—: \(error.localizedDescription)"
            input.removeTap(onBus: 0)
            self.sttRequest = nil
            return
        }

        // 5) èªè­˜ï¼ˆãƒ—ãƒ­ãƒˆã‚³ãƒ«çµŒç”±ï¼‰
        sttTask = speech.startTask(
            request: req,
            onResult: { [weak self] text, isFinal in
                Task { @MainActor in
                    self?.transcript = text
                    if isFinal { self?.stopLocalTranscription() }
                }
            },
            onError: { [weak self] err in
                guard let self else { return }
                
                // ã‚­ãƒ£ãƒ³ã‚»ãƒ«/ç„¡éŸ³ãªã©ã®"æ­£å¸¸çµ‚äº†æ‰±ã„"ã¯ UI ã«å‡ºã•ãªã„
                if self.userStoppedRecording || Self.isBenignSpeechError(err) {
                    Task { @MainActor in self.finishSTTCleanup() }
                    return
                }
                
                // ãã‚Œä»¥å¤–ã®ã¿ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
                Task { @MainActor in
                    self.errorMessage = err.localizedDescription
                    self.finishSTTCleanup()
                }
            }
        )

        isRecording = true
        mode = .localSTT
    }

    public func stopLocalTranscription() {
        guard isRecording || sttTask != nil else { return }
        userStoppedRecording = true                // ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã¦ã‹ã‚‰
        audioEngine.inputNode.removeTap(onBus: 0)
        audioEngine.stop()
        sttRequest?.endAudio()
        sttTask?.cancel()                          // ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã¯å¿…è¦ã€‚ä¸Šã§ç„¡å®³æ‰±ã„ã«ã™ã‚‹
        finishSTTCleanup()
    }

    // MARK: - Realtimeï¼ˆæ¬¡ã®æ®µéšï¼šä¼šè©±ï¼‰
    public func startRealtimeSession() {
        // æ—¢ã«æ¥ç¶šä¸­ã¾ãŸã¯æ¥ç¶šæ¸ˆã¿ã®å ´åˆã¯ä½•ã‚‚ã—ãªã„
        guard !isRealtimeActive && !isRealtimeConnecting else {
            print("âš ï¸ ConversationController: æ—¢ã«éŸ³å£°ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã¾ãŸã¯æ¥ç¶šä¸­ã§ã™")
            return
        }
        // æ—¢å­˜ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
        sessionStartTask?.cancel()
        startRealtimeSessionInternal()
    }
    
    private func startRealtimeSessionInternal() {
        // æ¥ç¶šä¸­ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
        isRealtimeConnecting = true
        nudgeCount = 0  // ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹æ™‚ã«ä¿ƒã—å›æ•°ã‚’ãƒªã‚»ãƒƒãƒˆ
        logNetworkEnvironment()
        
        // ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æ§‹æˆ
        do {
            try audioSessionManager.configure()
            
            // âœ… å…±é€šã‚¨ãƒ³ã‚¸ãƒ³ã‚’é–‹å§‹ï¼ˆAudioSessionè¨­å®šå¾Œï¼‰
            // âš ï¸ é‡è¦ãªé †åºï¼šAudioSessionã‚’è¨­å®šã—ã¦ã‹ã‚‰ã‚¨ãƒ³ã‚¸ãƒ³ã‚’é–‹å§‹
            // âœ… å…±é€šã‚¨ãƒ³ã‚¸ãƒ³ã‚’é–‹å§‹ã™ã‚‹ã“ã¨ã§ã€VoiceProcessingIOï¼ˆAECï¼‰ãŒå…¥å‡ºåŠ›ã®ä¸¡æ–¹ã«åŠ¹ã
            try sharedAudioEngine.start()
            print("âœ… ConversationController: å…±é€šAudioEngineé–‹å§‹æˆåŠŸï¼ˆAECæœ‰åŠ¹åŒ–ï¼‰")
            
            // AudioSessionè¨­å®šå¾Œã«PlayerNodeStreamerã®ã‚¨ãƒ³ã‚¸ãƒ³ã‚’é–‹å§‹
            try player.start()
            
            // éŸ³é‡ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            let audioSession = AVAudioSession.sharedInstance()
            print("ğŸ“Š ConversationController: ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªè¨­å®šç¢ºèª")
            print("   - OutputVolume: \(audioSession.outputVolume) (1.0ãŒæœ€å¤§)")
            print("   - OutputChannels: \(audioSession.outputNumberOfChannels)")
            print("   - SampleRate: \(audioSession.sampleRate)Hz")
            
            if audioSession.outputVolume < 0.1 {
                print("âš ï¸ ConversationController: éŸ³é‡ãŒéå¸¸ã«ä½ã„ã§ã™ï¼ˆ\(audioSession.outputVolume)ï¼‰ã€‚ãƒ‡ãƒã‚¤ã‚¹ã®éŸ³é‡è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            }
            
            print("âœ… ConversationController: AudioSessionè¨­å®šã¨PlayerNodeStreameré–‹å§‹æˆåŠŸ")
        } catch {
            self.errorMessage = "AudioSessionæ§‹æˆã«å¤±æ•—: \(error.localizedDescription)"
            print("âŒ ConversationController: AudioSessionæ§‹æˆå¤±æ•— - \(error.localizedDescription)")
            
            // è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
            if let nsError = error as NSError? {
                print("   - Error Domain: \(nsError.domain)")
                print("   - Error Code: \(nsError.code)")
                print("   - Error Info: \(nsError.userInfo)")
            }
        }

        let key = AppConfig.openAIKey
        print("ğŸ”‘ ConversationController: APIã‚­ãƒ¼ç¢ºèª - \(key.prefix(10))...")
        guard !key.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty else {
            self.errorMessage = "OPENAI_API_KEY ãŒæœªè¨­å®šã§ã™ï¼ˆSecrets.xcconfig ã‚’ç¢ºèªï¼‰"
            return
        }
        
        audioPreviewClient = AudioPreviewStreamingClient(
            apiKey: key,
            apiBase: URL(string: AppConfig.apiBase) ?? URL(string: "https://api.openai.com")!
        )
        
        // âœ… é‡è¦: Playerã®çŠ¶æ…‹å¤‰åŒ–ã‚’ç›£è¦–ã—ã¦ã€æ­£ç¢ºãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ãƒã‚¤ã‚¯ã®ã‚²ãƒ¼ãƒˆã‚’é–‹é–‰ã™ã‚‹
        player.onPlaybackStateChange = { [weak self] isPlaying in
            Task { @MainActor in
                guard let self = self else { return }
                guard let playbackId = self.playbackTurnId, playbackId == self.currentTurnId else {
                    if let playbackId = self.playbackTurnId {
                        print("â­ï¸ ConversationController: playback state change ignored for stale turn \(playbackId)")
                    }
                    if !isPlaying { self.playbackTurnId = nil }
                    return
                }
                self.isAIPlayingAudio = isPlaying
                self.isPlayingAudio = isPlaying
                self.mic?.setAIPlayingAudio(isPlaying)
                
                if isPlaying {
                    print("ğŸ”Š ConversationController: å†ç”Ÿé–‹å§‹ - ãƒã‚¤ã‚¯ã‚²ãƒ¼ãƒˆé–‰ (AEC/BargeInãƒ¢ãƒ¼ãƒ‰)")
                    if self.turnMetrics.firstAudio == nil {
                        self.turnMetrics.firstAudio = Date()
                        self.logTurnStageTiming(event: "firstAudio", at: self.turnMetrics.firstAudio!)
                    }
                } else {
                    print("ğŸ”‡ ConversationController: å†ç”Ÿå®Œå…¨çµ‚äº† - ãƒã‚¤ã‚¯ã‚²ãƒ¼ãƒˆé–‹")
                    // âœ… AIãŒå®Œå…¨ã«è©±ã—çµ‚ã‚ã£ãŸã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å¾…ã¤çŠ¶æ…‹ã«ã—ã¦ã‚¿ã‚¤ãƒãƒ¼ã‚’é–‹å§‹ï¼ˆãƒãƒ³ã‚ºãƒ•ãƒªãƒ¼æ™‚ã¯å³å†é–‹ï¼‰
                    if self.isHandsFreeMode && self.isRecording {
                        self.resumeListening()
                    } else if self.turnState == .speaking {
                        self.turnState = .waitingUser
                        print("â° ConversationController: AIã®éŸ³å£°å†ç”Ÿå®Œå…¨çµ‚äº† -> ä¿ƒã—ã‚¿ã‚¤ãƒãƒ¼ã‚’é–‹å§‹")
                        self.startWaitingForResponse()
                    }
                    self.turnMetrics.playbackEnd = Date()
                    // firstAudio ãŒæœªè¨­å®šã§ playbackEnd ãŒå…ˆã«æ¥ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    if self.turnMetrics.firstAudio == nil {
                        self.turnMetrics.firstAudio = self.turnMetrics.requestStart ?? self.turnMetrics.playbackEnd
                    }
                    if let end = self.turnMetrics.playbackEnd {
                        self.logTurnStageTiming(event: "playbackEnd", at: end)
                    }
                    self.logTurnLatencySummary(context: "playback complete")
                    self.playbackTurnId = nil
                }
            }
        }
        
        transcript = ""
        print("ğŸŸ¥ aiResponseText cleared at:", #function)
        aiResponseText = ""
        liveSummary = ""
        liveInterests = []
        liveNewVocabulary = []
        inMemoryTurns.removeAll()
        errorMessage = nil
        turnState = .waitingUser  // ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹æ™‚ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè©±ã™ã®ã‚’å¾…ã¤

        // âœ… Firebaseã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
        guard let userId = self.currentUserId, let childId = self.currentChildId else {
            print("âš ï¸ ConversationController: ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã§ãã¾ã›ã‚“")
            self.errorMessage = "ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚"
            return
        }
        
        let newSessionId = UUID().uuidString
        self.currentSessionId = newSessionId
        self.turnCount = 0
        conversationHistory.removeAll()
        
        // ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ«ãƒ¼ãƒˆå¤‰æ›´æ™‚ã«ã‚°ãƒ©ãƒ•ã‚’ç«‹ã¦ç›´ã™
        if routeChangeObserver == nil {
            routeChangeObserver = NotificationCenter.default.addObserver(
                forName: AVAudioSession.routeChangeNotification,
                object: nil,
                queue: .main
            ) { [weak self] note in
                self?.handleAudioRouteChange(note)
            }
        }
        
        let session = FirebaseConversationSession(
            id: newSessionId,
            mode: .freeTalk,
            startedAt: Date(),
            interestContext: [],
            summaries: [],
            newVocabulary: [],
            turnCount: 0
        )
        
        Task.detached { [weak self, userId, childId, session] in
            let repo = FirebaseConversationsRepository()
            do {
                try await repo.createSession(
                    userId: userId,
                    childId: childId,
                    session: session
                )
                print("âœ… ConversationController: Firebaseã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå®Œäº† - sessionId: \(session.id ?? "nil")")
            } catch {
                await MainActor.run {
                    self?.logFirebaseError(error, operation: "Firebaseã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ")
                }
            }
        }

        sessionStartTask = Task { [weak self] in
            guard let self else { return }
            do {
                print("ğŸš€ ConversationController: gpt-4o-audio-previewã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹")
                
                // çŠ¶æ…‹ã‚’æ›´æ–°
                await MainActor.run {
                    self.isRealtimeConnecting = false
                    self.isRealtimeActive = true
                    self.mode = .realtime
                    self.startWaitingForResponse()
                }
            } catch {
                print("âŒ ConversationController: ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹å¤±æ•— - \(error.localizedDescription)")
                await MainActor.run {
                    self.errorMessage = "éŸ³å£°ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ¥ç¶šå¤±æ•—: \(error.localizedDescription)"
                    self.isRealtimeConnecting = false
                    self.isRealtimeActive = false
                }
            }
        }
    }

    public func stopRealtimeSession() {
        print("ğŸ›‘ ConversationController: Realtimeã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†")
        ignoreIncomingAIChunks = true
        currentTurnId = 0
        listeningTurnId = 0
        playbackTurnId = nil
        
        // ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
        sessionStartTask?.cancel()
        sessionStartTask = nil
        
        // å—ä¿¡ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
        receiveTextTask?.cancel(); receiveTextTask = nil
        receiveAudioTask?.cancel(); receiveAudioTask = nil
        receiveInputTextTask?.cancel(); receiveInputTextTask = nil
        
        // ãƒã‚¤ã‚¯ã¨ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åœæ­¢
        mic?.stop(); mic = nil
        stopPlayer(reason: "stopRealtimeSession")
        isAIPlayingAudio = false
        isPlayingAudio = false
        
        // â˜… é‡è¦ï¼šå…ˆã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–ï¼ˆä»–ã‚¢ãƒ—ãƒªã¸ã‚‚é€šçŸ¥ï¼‰
        let s = AVAudioSession.sharedInstance()
        try? s.setActive(false, options: [.notifyOthersOnDeactivation])
        
        // â˜… ã‚¨ãƒ³ã‚¸ãƒ³ã‚’å®Œå…¨ã«è§£ä½“
        sharedAudioEngine.inputNode.removeTap(onBus: 0)   // å†ªç­‰
        if sharedAudioEngine.isRunning {
            sharedAudioEngine.stop()
        }
        sharedAudioEngine.reset()                          // â† ã“ã‚ŒãŒ2å›ç›®ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã®äºˆé˜²ç·š
        
        // âœ… ä¿ƒã—ã‚¿ã‚¤ãƒãƒ¼ã‚’åœæ­¢
        cancelNudge()
        silenceTimer?.invalidate()
        silenceTimer = nil
        
        // çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
        isRecording = false
        isHandsFreeMode = false
        isRealtimeActive = false
        isRealtimeConnecting = false
        turnState = .idle
        nudgeCount = 0
        liveSummaryTask?.cancel()
        liveSummaryTask = nil
        inMemoryTurns.removeAll()
        conversationHistory.removeAll()
        
        // ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢
        transcript = ""
        print("ğŸŸ¥ aiResponseText cleared at:", #function)
        aiResponseText = ""
        
        // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¯ãƒªã‚¢
        errorMessage = nil
        
        Task { [weak self] in
            guard let self else { 
                print("âš ï¸ ConversationController: stopRealtimeSession - selfãŒnilã®ãŸã‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return 
            }
            
            // âœ… Firebaseã«ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†ã‚’è¨˜éŒ²
            guard let userId = self.currentUserId, let childId = self.currentChildId, let sessionId = self.currentSessionId else {
                print("âš ï¸ ConversationController: stopRealtimeSession - ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã¾ãŸã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³IDãŒnilã®ãŸã‚ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return
            }
            print("ğŸ”„ ConversationController: stopRealtimeSession - ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†å‡¦ç†é–‹å§‹ - sessionId: \(sessionId)")
            let endedAt = Date()
            do {
                try await self.firebaseRepository.finishSession(
                    userId: userId,
                    childId: childId,
                    sessionId: sessionId,
                    endedAt: endedAt
                )
                print("âœ… ConversationController: Firebaseã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ›´æ–°å®Œäº† - sessionId: \(sessionId)")
                
                // âœ… ä¼šè©±çµ‚äº†å¾Œã®åˆ†æå‡¦ç†ã‚’å®Ÿè¡Œ
                print("ğŸ”„ ConversationController: stopRealtimeSession - åˆ†æå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ - sessionId: \(sessionId), turnCount=\(self.inMemoryTurns.count)")
                await self.analyzeSession(sessionId: sessionId)
                print("âœ… ConversationController: stopRealtimeSession - åˆ†æå‡¦ç†å®Œäº† - sessionId: \(sessionId)")
            } catch {
                print("âŒ ConversationController: stopRealtimeSession - ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: \(error)")
                self.logFirebaseError(error, operation: "Firebaseã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ›´æ–°")
            }
            
            await MainActor.run {
                self.currentSessionId = nil
                self.turnCount = 0
                print("âœ… ConversationController: ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
            }
        }
    }
    
    public func startPTTRealtime() {
        // ãƒãƒ³ã‚ºãƒ•ãƒªãƒ¼ä¸­ã«PTTã‚’é–‹å§‹ã—ãŸã‚‰ãƒãƒ³ã‚ºãƒ•ãƒªãƒ¼ã‚’æ˜ç¤ºçš„ã«ç„¡åŠ¹åŒ–
        if isHandsFreeMode {
            stopHandsFreeConversation()
        }
        guard audioPreviewClient != nil else {
            self.errorMessage = "éŸ³å£°ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"; return
        }
        // ä¿ƒã—ã¨æ—¢å­˜ã®éŒ²éŸ³ã‚’ãƒªã‚»ãƒƒãƒˆ
        cancelNudge()
        recordedPCMData.removeAll()
        recordedSampleRate = 24_000
        
        // ãƒãƒ¼ã‚¸ã‚¤ãƒ³å‰æã§AIéŸ³å£°ã‚’æ­¢ã‚ã‚‹
        stopPlayer(reason: "startPTTRealtime (barge-in before PTT)")
        playbackTurnId = nil
        isAIPlayingAudio = false
        isPlayingAudio = false
        mic?.setAIPlayingAudio(false)
        
        // å…±æœ‰ã‚¨ãƒ³ã‚¸ãƒ³ãŒæ­¢ã¾ã£ã¦ã„ãŸã‚‰å†é–‹
        if !sharedAudioEngine.isRunning {
            do { try sharedAudioEngine.start() } catch {
                print("âš ï¸ ConversationController: ã‚¨ãƒ³ã‚¸ãƒ³å†é–‹å¤±æ•— - \(error.localizedDescription)")
            }
        }
        
        startPTTRealtimeInternal()
    }
    
    private func startPTTRealtimeInternal() {
        cancelNudge()             // âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè©±ã—å§‹ã‚ã‚‹ã®ã§ä¿ƒã—ã‚’æ­¢ã‚ã‚‹
        // ğŸ”‡ ã„ã¾æµã‚Œã¦ã„ã‚‹AIéŸ³å£°ã‚’æ­¢ã‚ã‚‹ï¼ˆbarge-in å‰æï¼‰
        stopPlayer(reason: "startPTTRealtimeInternal (barge-in before reconfigure)")
        playbackTurnId = nil
        isAIPlayingAudio = false
        isPlayingAudio = false
        mic?.setAIPlayingAudio(false)
        ignoreIncomingAIChunks = false

        mic?.stop()
        
        // âœ… Taskå†…ã§å®Ÿè¡Œ
        Task { @MainActor [weak self] in
            guard let self = self else { return }
            
            // 1. ã‚¨ãƒ³ã‚¸ãƒ³ãŒã„ã£ãŸã‚“å‹•ã„ã¦ã„ã‚‹ãªã‚‰åœæ­¢ï¼ˆå†æ§‹æˆã®ãŸã‚ï¼‰
            // VoiceProcessingIOã¯æ§‹æˆå¤‰æ›´æ™‚ã«åœæ­¢ã—ã¦ã„ã‚‹ã®ãŒæœ€ã‚‚å®‰å…¨
            if self.sharedAudioEngine.isRunning {
                self.sharedAudioEngine.stop()
            }
            
            // 2. ãƒã‚¤ã‚¯ã‚’åˆæœŸåŒ– & Start (ã“ã“ã§ Tap ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)
            // ã‚¨ãƒ³ã‚¸ãƒ³ã¯åœæ­¢çŠ¶æ…‹ã ãŒã€Tapã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¯å¯èƒ½
            self.mic = MicrophoneCapture(sharedEngine: self.sharedAudioEngine, onPCM: { [weak self] buf in
                self?.appendPCMBuffer(buf)
            }, outputMonitor: self.player.outputMonitor)
            self.mic?.onBargeIn = { [weak self] in
                self?.interruptAI()
            }
            
            // -------------------------------------------------------
            // âœ… è¿½åŠ : ãƒã‚¤ã‚¯ã®éŸ³é‡ã‚’ç›£è¦–ã—ã¦ã€éŸ³ãŒã—ã¦ã„ã‚Œã°æ™‚åˆ»ã‚’æ›´æ–°
            // -------------------------------------------------------
            self.mic?.onVolume = { [weak self] rms in
                // ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰å‘¼ã°ã‚Œã‚‹ãŸã‚MainActorã¸
                Task { @MainActor [weak self] in
                    guard let self = self else { return }
                    self.lastInputRMS = rms
                    
                    // é–¾å€¤(-50dB)ã‚ˆã‚Šå¤§ãã‘ã‚Œã°ã€ŒéŸ³ãŒã—ã¦ã„ã‚‹ã€ã¨ã¿ãªã—ã¦æ™‚åˆ»æ›´æ–°
                    if rms > self.silenceThresholdDb {
                        self.lastUserVoiceActivityTime = Date()
                    }
                }
            }
            self.lastInputRMS = nil
            
            // ãƒã‚¤ã‚¯é–‹å§‹æ™‚ã«æ™‚åˆ»ã‚’ãƒªã‚»ãƒƒãƒˆ
            self.lastUserVoiceActivityTime = Date()
            
            do {
                // ã“ã“ã§ Tap ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã‚‹ (ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè£œæ­£è¾¼ã¿)
                try self.mic?.start()
        } catch {
                self.errorMessage = "ãƒã‚¤ã‚¯è¨­å®šå¤±æ•—: \(error.localizedDescription)"
                self.isRecording = false
                return
            }
            
            // 3. ãã®å¾Œã§ã€ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ Prepare & Start
            self.sharedAudioEngine.prepare()
            do {
                try self.sharedAudioEngine.start()
                print("âœ… ConversationController: ã‚¨ãƒ³ã‚¸ãƒ³å†é–‹æˆåŠŸ")
            } catch {
                print("âŒ ConversationController: ã‚¨ãƒ³ã‚¸ãƒ³é–‹å§‹å¤±æ•—: \(error)")
                self.errorMessage = "ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¨ãƒ³ã‚¸ãƒ³ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ"
                self.isRecording = false
                return
            }
            
            // 4. çŠ¶æ…‹æ›´æ–°
            self.isRecording = true
            self.transcript = ""
            self.turnState = .listening
            self.markListeningTurn()
            self.turnMetrics = TurnMetrics()
            self.turnMetrics.listenStart = Date()
            print("â±ï¸ Latency: listen start (PTT) at \(self.turnMetrics.listenStart!)")
            
            // æ³¨æ„: ä¿ƒã—ã‚¿ã‚¤ãƒãƒ¼ã¯ onResponseDone ã§ã®ã¿ã‚»ãƒƒãƒˆã™ã‚‹ï¼ˆAIå¿œç­”å®Œäº†æ™‚ã®ã¿ï¼‰
            // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè©±ã—å§‹ã‚ãŸç›´å¾Œã¯ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚»ãƒƒãƒˆã—ãªã„
            print("âœ… ConversationController: PTTé–‹å§‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å®Œäº†")
        }
    }
    

    public func stopPTTRealtime() {
        isRecording = false
        isHandsFreeMode = false
        mic?.stop()
        if turnMetrics.speechEnd == nil {
            turnMetrics.speechEnd = Date()
        }
        turnState = .thinking
        Task { [weak self] in
            await self?.sendAudioPreviewRequest()
        }
    }
    
    // MARK: - Hands-free Conversation (VAD)
    /// 1å›ã®ã‚¿ãƒƒãƒ—ã§ã€Œèãâ†’é€ã‚‹â†’AIå¿œç­”â†’å†é–‹ã€ã‚’ç¹°ã‚Šè¿”ã™ãƒ¢ãƒ¼ãƒ‰
    public func startHandsFreeConversation() {
        guard audioPreviewClient != nil else {
            self.errorMessage = "ã¾ãšã€Œé–‹å§‹ã€ã‚’æŠ¼ã—ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹ã„ã¦ãã ã•ã„"
            return
        }
        // PTTéŒ²éŸ³ä¸­ã«åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œãªã„ã‚ˆã†ã‚¬ãƒ¼ãƒ‰
        if isRecording && !isHandsFreeMode {
            self.errorMessage = "ç¾åœ¨ã®éŒ²éŸ³ã‚’åœæ­¢ã—ã¦ã‹ã‚‰ãƒãƒ³ã‚ºãƒ•ãƒªãƒ¼ã‚’é–‹å§‹ã—ã¦ãã ã•ã„"
            return
        }
        
        cancelNudge()
        recordedPCMData.removeAll()
        recordedSampleRate = 24_000
        isUserSpeaking = false
        silenceTimer?.invalidate()
        silenceTimer = nil
        
        // ãƒãƒ¼ã‚¸ã‚¤ãƒ³å‰æã§AIéŸ³å£°ã‚’æ­¢ã‚ã‚‹
        stopPlayer(reason: "startHandsFreeConversation (barge-in before HF start)")
        playbackTurnId = nil
        isAIPlayingAudio = false
        isPlayingAudio = false
        mic?.setAIPlayingAudio(false)
        
        if !sharedAudioEngine.isRunning {
            do { try sharedAudioEngine.start() } catch {
                print("âš ï¸ ConversationController: ã‚¨ãƒ³ã‚¸ãƒ³å†é–‹å¤±æ•— - \(error.localizedDescription)")
            }
        }
        
        isHandsFreeMode = true
        vadState = .idle
        speechStartTime = nil
        silenceTimer?.invalidate()
        silenceTimer = nil
        startHandsFreeInternal()
    }
    
    public func stopHandsFreeConversation() {
        silenceTimer?.invalidate()
        silenceTimer = nil
        isHandsFreeMode = false
        isRecording = false
        isUserSpeaking = false
        vadState = .idle
        speechStartTime = nil
        recordedPCMData.removeAll()
        mic?.stop()
        turnState = .waitingUser
    }
    
    private func startHandsFreeInternal() {
        cancelNudge()
        stopPlayer(reason: "startHandsFreeInternal (reset before VAD start)")
        playbackTurnId = nil
        isAIPlayingAudio = false
        isPlayingAudio = false
        mic?.setAIPlayingAudio(false)
        mic?.stop()
        ignoreIncomingAIChunks = false
        
        Task { @MainActor [weak self] in
            guard let self = self else { return }
            
            // 1. ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä¸€åº¦æ­¢ã‚ã¦ã‹ã‚‰å†æ§‹æˆ
            if self.sharedAudioEngine.isRunning {
                self.sharedAudioEngine.stop()
            }
            
            self.mic = MicrophoneCapture(sharedEngine: self.sharedAudioEngine, onPCM: { [weak self] buf in
                self?.appendPCMBuffer(buf)
            }, outputMonitor: self.player.outputMonitor)
            
            self.mic?.onBargeIn = { [weak self] in
                self?.interruptAI()
            }
            self.mic?.onVADProbability = { [weak self] probability in
                Task { @MainActor [weak self] in
                    self?.handleVAD(probability: probability)
                }
            }
            self.mic?.onVolume = { [weak self] rms in
                Task { @MainActor [weak self] in
                    guard let self else { return }
                    self.lastInputRMS = rms
                    
                    if rms > self.silenceThresholdDb {
                        self.lastUserVoiceActivityTime = Date()
                    }
                }
            }
            self.lastInputRMS = nil
            
            recordedPCMData.removeAll()
            recordedSampleRate = 24_000
            
            do {
                try self.mic?.start()
            } catch {
                self.errorMessage = "ãƒã‚¤ã‚¯è¨­å®šå¤±æ•—: \(error.localizedDescription)"
                self.isRecording = false
                self.isHandsFreeMode = false
                return
            }
            
            self.sharedAudioEngine.prepare()
            do {
                try self.sharedAudioEngine.start()
                print("âœ… ConversationController: ãƒãƒ³ã‚ºãƒ•ãƒªãƒ¼ç”¨ã‚¨ãƒ³ã‚¸ãƒ³é–‹å§‹")
            } catch {
                print("âŒ ConversationController: ã‚¨ãƒ³ã‚¸ãƒ³é–‹å§‹å¤±æ•—: \(error)")
                self.errorMessage = "ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¨ãƒ³ã‚¸ãƒ³ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ"
                self.isRecording = false
                self.isHandsFreeMode = false
                return
            }
            
            self.isRecording = true
            self.turnState = .listening
            self.markListeningTurn()
            self.vadState = .idle
            self.speechStartTime = nil
            self.turnMetrics = TurnMetrics()
            self.turnMetrics.listenStart = Date()
            print("â±ï¸ Latency: listen start (handsfree) at \(self.turnMetrics.listenStart!)")
            print("ğŸŸ¢ ãƒãƒ³ã‚ºãƒ•ãƒªãƒ¼ä¼šè©±é–‹å§‹: Listening...")
        }
    }

    private func handleVAD(probability: Float) {
        // AIå†ç”Ÿä¸­ã®å‰²ã‚Šè¾¼ã¿åˆ¤å®šï¼šSilero VAD ã‚’ä¸€å®šæ™‚é–“é€£ç¶šæ¤œå‡ºã—ãŸå ´åˆã®ã¿åœæ­¢
        if isAIPlayingAudio && turnState == .speaking {
            if probability >= bargeInVADThreshold {
                if vadInterruptSpeechStart == nil {
                    vadInterruptSpeechStart = Date()
                } else if let start = vadInterruptSpeechStart,
                          Date().timeIntervalSince(start) >= bargeInHoldDuration {
                    vadInterruptSpeechStart = nil
                    interruptAI()
                    return
                }
            } else {
                vadInterruptSpeechStart = nil
            }
        } else {
            vadInterruptSpeechStart = nil
        }
        
        if turnState == .thinking { return }
        guard listeningTurnId == currentTurnId else { return }
        
        switch vadState {
        case .idle:
            let now = Date()
            let rmsDb = lastInputRMS ?? -120.0
            let probTriggered = probability > speechStartThreshold
            let rmsTriggered = rmsDb > activeRmsStartThresholdDb
            if probTriggered || rmsTriggered {
                if speechStartCandidateTime == nil {
                    speechStartCandidateTime = now
                }
                let elapsed = now.timeIntervalSince(speechStartCandidateTime ?? now)
                if elapsed < speechStartHoldDuration {
                    return
                }
                speechStartCandidateTime = nil
                
                vadState = .speaking
                isUserSpeaking = true
                speechStartTime = Date()
                silenceTimer?.invalidate()
                silenceTimer = nil
                turnMetrics.listenStart = turnMetrics.listenStart ?? speechStartTime
                let probText = String(format: "%.4f", probability)
                let rmsText = String(format: "%.2f", rmsDb)
                let rmsStartText = String(format: "%.1f", activeRmsStartThresholdDb)
                let session = AVAudioSession.sharedInstance()
                let input = session.currentRoute.inputs.first
                let inputName = input?.portName ?? input?.portType.rawValue ?? "none"
                let preferredInput = session.preferredInput?.portName ?? session.preferredInput?.portType.rawValue ?? "none"
                print("â±ï¸ Latency: speech start detected at \(speechStartTime!) | prob=\(probText), rms=\(rmsText)dB, input=\(inputName), preferredInput=\(preferredInput), trigProb=\(probTriggered), trigRMS=\(rmsTriggered), rmsStartThresh=\(rmsStartText)")
            } else {
                speechStartCandidateTime = nil
            }
        case .speaking:
            let rmsDb = lastInputRMS ?? -120.0
            let isSilent = probability < speechEndThreshold || rmsDb < activeSpeechEndRmsThresholdDb
            if isSilent {
                if silenceTimer == nil {
                    let turnId = listeningTurnId
                    silenceTimer = Timer.scheduledTimer(withTimeInterval: activeMinSilenceDuration, repeats: false) { [weak self] _ in
                        Task { @MainActor [weak self] in
                            self?.handleSilenceTimeout(for: turnId)
                        }
                    }
                }
            } else {
                silenceTimer?.invalidate()
                silenceTimer = nil
            }
        }
    }
    private func handleSilenceTimeout(for turnId: Int) {
        guard turnId == currentTurnId, turnId == listeningTurnId else { return }
        guard vadState == .speaking else { return }
        silenceTimer?.invalidate()
        silenceTimer = nil
        
        let now = Date()
        let speechBegan = speechStartTime ?? now
        let duration = now.timeIntervalSince(speechBegan)
        speechStartTime = nil
        turnMetrics.speechEnd = now
        if let listenStart = turnMetrics.listenStart {
            let totalListen = now.timeIntervalSince(listenStart)
            print("â±ï¸ Latency: speech end (listen->speechEnd=\(String(format: "%.2f", totalListen))s, speechDuration=\(String(format: "%.2f", duration))s)")
        } else {
            print("â±ï¸ Latency: speech end (duration=\(String(format: "%.2f", duration))s)")
        }
        
        if duration < minSpeechDuration {
            vadState = .idle
            isUserSpeaking = false
            recordedPCMData.removeAll()
            let formattedDuration = String(format: "%.2f", duration)
            print("ğŸª« çŸ­ã™ãã‚‹ç™ºè©±ã‚’ç ´æ£„ (duration=\(formattedDuration)s)")
            return
        }
        
        vadState = .idle
        commitUserSpeech()
    }
    
    private func interruptAI() {
        guard turnState == .speaking else { return }
        print("âš¡ï¸ å‰²ã‚Šè¾¼ã¿æ¤œçŸ¥: AIåœæ­¢ -> èãå–ã‚Šã¸")
        vadInterruptSpeechStart = nil
        ignoreIncomingAIChunks = true
        
        // å†ç”Ÿã‚’å³æ™‚åœæ­¢ã—ã€ãƒã‚¤ã‚¯ã‚²ãƒ¼ãƒˆã‚’é–‹ã
        stopPlayer(reason: "interruptAI (barge-in)")
        playbackTurnId = nil
        isAIPlayingAudio = false
        isPlayingAudio = false
        mic?.setAIPlayingAudio(false)
        
        // ã‚¹ãƒ†ãƒ¼ãƒˆã‚’Listeningã¸æˆ»ã—ã€ãƒãƒƒãƒ•ã‚¡ã‚’æ–°è¦ç™ºè©±ç”¨ã«ã™ã‚‹
        turnState = .listening
        markListeningTurn()
        recordedPCMData.removeAll()
        isUserSpeaking = true
        turnMetrics = TurnMetrics()
        turnMetrics.listenStart = Date()
        print("â±ï¸ Latency: listen start (barge-in) at \(turnMetrics.listenStart!)")
        silenceTimer?.invalidate()
        silenceTimer = nil
        
        // ãƒã‚¤ã‚¯ãŒæ­¢ã¾ã£ã¦ã„ã‚Œã°å†é–‹
        if !sharedAudioEngine.isRunning {
            try? sharedAudioEngine.start()
        }
        do {
            try mic?.start()
        } catch {
            errorMessage = "ãƒã‚¤ã‚¯å†é–‹ã«å¤±æ•—ã—ã¾ã—ãŸ: \(error.localizedDescription)"
            isRecording = false
            isHandsFreeMode = false
        }
    }
    
    private func commitUserSpeech() {
        guard isHandsFreeMode else { return }
        guard isUserSpeaking else { return }
        print("ğŸš€ ç™ºè©±çµ‚äº†åˆ¤å®š -> ã‚²ãƒ¼ãƒˆé€šéãƒã‚§ãƒƒã‚¯")
        
        isUserSpeaking = false
        silenceTimer?.invalidate()
        silenceTimer = nil
        
        let audioData = recordedPCMData
        recordedPCMData.removeAll()
        let sampleRate = recordedSampleRate > 0 ? recordedSampleRate : 16_000
        let durationSec = Double(audioData.count) / 2.0 / sampleRate
        guard durationSec >= 0.2 else {
            let formatted = String(format: "%.2f", durationSec)
            print("ğŸ§ User speech too short, discarding (\(formatted)s)")
            return
        }
        
        let wavData = pcm16ToWav(pcmData: audioData, sampleRate: sampleRate)
        
        Task { [weak self] in
            guard let self else { return }
            
            let transcript = await ConversationController.transcribeUserAudio(wavData: wavData)
            let cleaned = transcript?.trimmingCharacters(in: .whitespacesAndNewlines) ?? ""
            let formattedDuration = String(format: "%.2f", durationSec)
            if cleaned.isEmpty || cleaned == "(voice)" {
                print("ğŸ” Local STT returned empty, skip AI request (duration=\(formattedDuration)s, bytes=\(audioData.count))")
            } else {
                print("ğŸ“ Local STT succeeded: '\(cleaned)' (duration=\(formattedDuration)s)")
            }
            
            await MainActor.run {
                self.mic?.stop()
                self.turnState = .thinking
                if self.turnMetrics.speechEnd == nil { self.turnMetrics.speechEnd = Date() }
                if let listenStart = self.turnMetrics.listenStart, let speechEnd = self.turnMetrics.speechEnd {
                    print("â±ï¸ Latency: capture done (listen->speechEnd=\(String(format: "%.2f", speechEnd.timeIntervalSince(listenStart)))s)")
                }
            }
            
            if !cleaned.isEmpty, cleaned != "(voice)" {
                await self.sendTextPreviewRequest(userText: cleaned)
            } else {
                await self.persistVoiceOnlyTurn()
                await MainActor.run {
                    self.isThinking = false
                    self.turnState = .waitingUser
                    if self.isHandsFreeMode && self.isRecording {
                        self.resumeListening()
                    }
                }
            }
        }
    }
    
    private func resumeListening() {
        guard isHandsFreeMode else { return }
        print("ğŸ‘‚ èãå–ã‚Šå†é–‹")
        turnState = .listening
        markListeningTurn()
        recordedPCMData.removeAll()
        isUserSpeaking = false
        turnMetrics = TurnMetrics()
        turnMetrics.listenStart = Date()
        print("â±ï¸ Latency: listen start (resume) at \(turnMetrics.listenStart!)")
        silenceTimer?.invalidate()
        silenceTimer = nil
        ignoreIncomingAIChunks = false
        
        if !sharedAudioEngine.isRunning {
            try? sharedAudioEngine.start()
        }
        
        do {
            try mic?.start()
        } catch {
            errorMessage = "ãƒã‚¤ã‚¯å†é–‹ã«å¤±æ•—ã—ã¾ã—ãŸ: \(error.localizedDescription)"
            isRecording = false
            isHandsFreeMode = false
        }
    }

    private func startReceiveLoops() {
        print("ğŸ”„ ConversationController: startReceiveLoopsã¯gpt-4o-audio-previewãƒ¢ãƒ¼ãƒ‰ã§ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ï¼ˆRealtime APIã‚³ãƒ¼ãƒ‰ã‚’ç„¡åŠ¹åŒ–ï¼‰")
    }
    
    // MARK: - Private Helpers
    private func advanceTurnId() -> Int {
        currentTurnId += 1
        playbackTurnId = nil
        return currentTurnId
    }
    
    private func markListeningTurn() {
        listeningTurnId = currentTurnId
        silenceTimer?.invalidate()
        silenceTimer = nil
    }
    
    private func isCurrentTurn(_ turnId: Int) -> Bool {
        turnId == currentTurnId
    }
    
    private func appendPCMBuffer(_ buffer: AVAudioPCMBuffer) {
        guard let channelData = buffer.int16ChannelData else { return }
        let frameLength = Int(buffer.frameLength)
        let channels = Int(buffer.format.channelCount)
        let bytesPerFrame = channels * MemoryLayout<Int16>.size
        let byteCount = frameLength * bytesPerFrame
        let data = Data(bytes: channelData[0], count: byteCount)
        recordedPCMData.append(data)
        recordedSampleRate = buffer.format.sampleRate
    }

    private func stopPlayer(reason: String, function: String = #function) {
        let playbackIdText = playbackTurnId.map(String.init) ?? "nil"
        print("ğŸ›‘ PlayerNodeStreamer.stop() call - caller=\(function), reason=\(reason), playbackTurnId=\(playbackIdText), currentTurnId=\(currentTurnId), turnState=\(turnState)")
        player.stop()
    }

    // âœ… ç›¸æ§Œã‚’ãƒ©ãƒ³ãƒ€ãƒ å†ç”Ÿã™ã‚‹
    private func playRandomFiller() {
        guard enableFillers else {
            isFillerPlaying = false
            return
        }
        guard let fileName = fillerFiles.randomElement(),
              let url = Bundle.main.url(forResource: fileName, withExtension: "wav") else {
            print("âš ï¸ ç›¸æ§Œãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: \(fillerFiles)")
            return
        }
        print("ğŸ—£ï¸ ç›¸æ§Œå†ç”Ÿ: \(fileName)")
        player.playLocalFile(url)
        isFillerPlaying = true
    }
    
    private func pcm16ToWav(pcmData: Data, sampleRate: Double) -> Data {
        // ã‚·ãƒ³ãƒ—ãƒ«ãªPCM16(LE)/ãƒ¢ãƒãƒ©ãƒ« -> WAVãƒ˜ãƒƒãƒ€ãƒ¼ä»˜ä¸
        var wav = Data()
        let numChannels: UInt16 = 1
        let bitsPerSample: UInt16 = 16
        let byteRate = UInt32(sampleRate) * UInt32(numChannels) * UInt32(bitsPerSample / 8)
        let blockAlign = UInt16(numChannels * bitsPerSample / 8)
        let dataSize = UInt32(pcmData.count)
        
        func appendLE<T: FixedWidthInteger>(_ value: T) {
            var le = value.littleEndian
            withUnsafeBytes(of: &le) { wav.append(contentsOf: $0) }
        }
        
        wav.append("RIFF".data(using: .ascii)!)
        appendLE(UInt32(36 + dataSize))
        wav.append("WAVE".data(using: .ascii)!)
        wav.append("fmt ".data(using: .ascii)!)
        appendLE(UInt32(16))           // PCM fmt chunk size
        appendLE(UInt16(1))            // PCM format
        appendLE(numChannels)
        appendLE(UInt32(sampleRate))
        appendLE(byteRate)
        appendLE(blockAlign)
        appendLE(bitsPerSample)
        wav.append("data".data(using: .ascii)!)
        appendLE(dataSize)
        wav.append(pcmData)
        return wav
    }
    
    /// ãƒ¦ãƒ¼ã‚¶ãƒ¼éŸ³å£°ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§æ–‡å­—èµ·ã“ã—ï¼ˆä¼šè©±å±¥æ­´ç”¨ï¼‰ã€‚å¤±æ•—æ™‚ã¯ nil ã‚’è¿”ã™ã€‚
    nonisolated private static func transcribeUserAudio(wavData: Data) async -> String? {
        // âœ… ã‚ªãƒ•ãƒ©ã‚¤ãƒ³STTãŒä¸å®‰å®šãªç’°å¢ƒã§ã¯æ—©æœŸã«è«¦ã‚ã¦ã‚¨ãƒ©ãƒ¼ã‚’æŠ‘åˆ¶
        let authStatus = SFSpeechRecognizer.authorizationStatus()
        guard authStatus == .authorized,
              let recognizer = SFSpeechRecognizer(locale: Locale(identifier: "ja-JP")),
              recognizer.isAvailable else { return nil }
        
        let tmpURL = FileManager.default.temporaryDirectory.appendingPathComponent("user_audio_\(UUID().uuidString).wav")
        do {
            try wavData.write(to: tmpURL)
        } catch {
            print("âš ï¸ UserAudioTranscribe: write failed - \(error)")
            return nil
        }
        
        return await withCheckedContinuation { continuation in
            var didFinish = false
            func finish(_ text: String?) {
                if didFinish { return }
                didFinish = true
                continuation.resume(returning: text)
                try? FileManager.default.removeItem(at: tmpURL)
            }
            
            let request = SFSpeechURLRecognitionRequest(url: tmpURL)
            var task: SFSpeechRecognitionTask?
            task = recognizer.recognitionTask(with: request) { result, error in
                if let result = result, result.isFinal {
                    finish(result.bestTranscription.formattedString)
                    task?.cancel()
                } else if let error = error {
                    // kAFAssistantErrorDomain 1101 ã¯ã€Œãƒ­ãƒ¼ã‚«ãƒ«èªè­˜ä¸å¯ã€ã§é »ç™ºã™ã‚‹ãŸã‚é™ã‹ã«ç„¡è¦–
                    let nsError = error as NSError
                    if !(nsError.domain == "kAFAssistantErrorDomain" && nsError.code == 1101) {
                        print("âš ï¸ UserAudioTranscribe: recognition error - \(error)")
                    }
                    finish(nil)
                }
            }
        }
    }
    
    private func sendTextPreviewRequest(userText: String) async {
        guard let client = audioPreviewClient else {
            await MainActor.run { self.errorMessage = "éŸ³å£°ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“" }
            return
        }
        
        let trimmed = userText.trimmingCharacters(in: .whitespacesAndNewlines)
        guard !trimmed.isEmpty else { return }

        await MainActor.run {
            // æ–°ã—ã„è¿”ç­”ã‚’é–‹å§‹ã™ã‚‹ã®ã§å‰ã®è¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢
            self.aiResponseText = ""
            self.turnMetrics.requestStart = Date()
            self.logTurnStageTiming(event: "request", at: self.turnMetrics.requestStart!)
        }

        let turnId = advanceTurnId()
        ignoreIncomingAIChunks = false

        // AIãŒè€ƒãˆå§‹ã‚ã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ç›¸æ§Œã‚’æ‰“ã¤
        await MainActor.run {
            self.isThinking = true
            self.playRandomFiller()
            self.player.prepareForNextStream()
        }

        let tStart = Date()
        print("â±ï¸ ConversationController: sendTextPreviewRequest start - textLen=\(trimmed.count)")
        
        do {
            let result = try await client.streamResponseText(
                userText: trimmed,
                systemPrompt: currentSystemPrompt,
                history: conversationHistory,
                onText: { [weak self] delta in
                    guard let self else { return }
                    Task { @MainActor in
                        guard !self.ignoreIncomingAIChunks, self.isCurrentTurn(turnId) else { return }
                        if self.turnMetrics.firstText == nil {
                            self.turnMetrics.firstText = Date()
                            self.logTurnStageTiming(event: "firstText", at: self.turnMetrics.firstText!)
                        }
                        let clean = self.sanitizeAIText(delta)
                        if clean.isEmpty { return }
                        print("ğŸŸ¦ onText delta (clean):", clean)
                        self.aiResponseText += clean
                    }
                },
                onAudioChunk: { [weak self] chunk in
                    guard let self else { return }
                    Task { @MainActor in
                        guard !self.ignoreIncomingAIChunks, self.isCurrentTurn(turnId) else { return }
                        print("ğŸ”Š onAudioChunk bytes:", chunk.count)
                        self.playbackTurnId = turnId
                        self.turnState = .speaking
                        // ç›¸æ§ŒãŒé³´ã£ã¦ã„ã¦ã‚‚å¼·åˆ¶åœæ­¢ã›ãšè‡ªç„¶ã«çµ‚ã‚ã‚‰ã›ã‚‹
                        if self.isFillerPlaying {
                            self.isFillerPlaying = false
                        }
                        self.handleFirstAudioChunk(for: turnId)
                        self.player.resumeIfNeeded()
                        // å‡ºåŠ›ã¯pcm16æŒ‡å®šãªã®ã§ãã®ã¾ã¾å†ç”Ÿ
                        self.player.playChunk(chunk)
                    }
                },
                onFirstByte: { [weak self] firstByte in
                    guard let self else { return }
                    Task { @MainActor in
                        guard !self.ignoreIncomingAIChunks, self.isCurrentTurn(turnId) else { return }
                        self.turnMetrics.firstByte = firstByte
                        self.logTurnStageTiming(event: "firstByte", at: firstByte)
                    }
                }
            )
            let tEnd = Date()
            print("â±ï¸ ConversationController: sendTextPreviewRequest completed in \(String(format: "%.2f", tEnd.timeIntervalSince(tStart)))s, finalText.count=\(result.text.count), finalText=\"\(result.text)\", audioMissing=\(result.audioMissing)")
            let cleanFinal = self.sanitizeAIText(result.text)
            guard self.isCurrentTurn(turnId) else { return }
            turnMetrics.streamComplete = tEnd
            logTurnStageTiming(event: "streamComplete", at: tEnd)
            logTurnLatencySummary(context: "stream complete (text)")
            
            await MainActor.run {
                // å¹ãå‡ºã—ç”¨ã«æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆã‚’UIã¸åæ˜ 
                self.aiResponseText = cleanFinal
                if !result.audioMissing {
                    if self.isHandsFreeMode && self.isRecording && self.turnState != .speaking {
                        self.resumeListening()
                    } else if self.turnState != .speaking {
                        self.turnState = .waitingUser
                        self.startWaitingForResponse()
                    }
                } else {
                    print("ğŸº ConversationController: text-only response -> fallback TTS will run")
                }
            }
            
            if result.audioMissing {
                await self.playFallbackTTS(text: cleanFinal, turnId: turnId)
            }
            
            // Firebaseä¿å­˜
            if let userId = currentUserId, let childId = currentChildId, let sessionId = currentSessionId {
                let userTurn = FirebaseTurn(role: .child, text: trimmed, timestamp: Date())
                let aiTurn = FirebaseTurn(role: .ai, text: cleanFinal, timestamp: Date())
                print("ğŸ—‚ï¸ ConversationController: append inMemoryTurns (user:'\(userTurn.text ?? "nil")', ai:'\(aiTurn.text ?? "nil")')")
                inMemoryTurns.append(contentsOf: [userTurn, aiTurn])
                // âœ… å±¥æ­´ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ç©ã‚€ï¼ˆç›´è¿‘ã®æ–‡è„ˆã¨ã—ã¦ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹APIã¸æ¸¡ã™ï¼‰
                conversationHistory.append(HistoryItem(role: "user", text: trimmed))
                conversationHistory.append(HistoryItem(role: "assistant", text: cleanFinal))
                // å±¥æ­´ãŒé•·ããªã‚Šã™ããªã„ã‚ˆã†ã«6ã‚¿ãƒ¼ãƒ³åˆ†ï¼ˆ12ã‚¨ãƒ³ãƒˆãƒªï¼‰ã«æŠ‘ãˆã‚‹
                if conversationHistory.count > 12 {
                    conversationHistory.removeFirst(conversationHistory.count - 12)
                }
                turnCount += 2
                let updatedTurnCount = turnCount
                Task.detached { [weak self, userId, childId, sessionId, userTurn, aiTurn, updatedTurnCount] in
                    let repo = FirebaseConversationsRepository()
                    do {
                        try await repo.addTurn(userId: userId, childId: childId, sessionId: sessionId, turn: userTurn)
                        try await repo.addTurn(userId: userId, childId: childId, sessionId: sessionId, turn: aiTurn)
                        try? await repo.updateTurnCount(userId: userId, childId: childId, sessionId: sessionId, turnCount: updatedTurnCount)
                    } catch {
                        await MainActor.run {
                            self?.logFirebaseError(error, operation: "ãƒ†ã‚­ã‚¹ãƒˆä¼šè©±ã®ä¿å­˜")
                        }
                    }
                }
                
                // å„ã‚¿ãƒ¼ãƒ³ã®çµ‚äº†æ™‚ã«ãƒ©ã‚¤ãƒ–è¦ç´„/ã‚¿ã‚°/æ–°èªã‚’ç”Ÿæˆã—ã¦å³æ™‚ä¿å­˜
                liveSummaryTask?.cancel()
                liveSummaryTask = Task { [weak self] in
                    print("ğŸ“ ConversationController: live analysis task start (turnCount=\(self?.inMemoryTurns.count ?? 0))")
                    await self?.generateLiveAnalysisAndPersist()
                    print("ğŸ“ ConversationController: live analysis task end (summary='\(self?.liveSummary ?? "")', interests=\(self?.liveInterests.map { $0.rawValue } ?? []), newWords=\(self?.liveNewVocabulary ?? []))")
                }
            }
        } catch {
            print("âŒ ConversationController: sendTextPreviewRequest failed - \(error)")
            await MainActor.run {
                guard self.isCurrentTurn(turnId) else { return }
                self.errorMessage = error.localizedDescription
                if self.isHandsFreeMode && self.isRecording {
                    self.resumeListening()
                } else {
                    self.turnState = .waitingUser
                }
            }
        }
        
        await MainActor.run {
            if self.isCurrentTurn(turnId) {
                self.isThinking = false
            }
        }
    }
    
    private func sendAudioPreviewRequest() async {
        guard let client = audioPreviewClient else {
            await MainActor.run { self.errorMessage = "éŸ³å£°ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“" }
            return
        }

        await MainActor.run {
            // æ–°ã—ã„è¿”ç­”ã‚’é–‹å§‹ã™ã‚‹ã®ã§å‰ã®è¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢
            self.aiResponseText = ""
            self.turnMetrics.requestStart = Date()
            self.logTurnStageTiming(event: "request", at: self.turnMetrics.requestStart!)
        }
        
        let captured = recordedPCMData
        recordedPCMData.removeAll()
        guard !captured.isEmpty else {
            await MainActor.run {
                self.errorMessage = "éŸ³å£°ãŒéŒ²éŸ³ã•ã‚Œã¦ã„ã¾ã›ã‚“"
                if self.isHandsFreeMode && self.isRecording {
                    self.resumeListening()
                } else {
                    self.turnState = .waitingUser
                }
            }
            return
        }

        let turnId = advanceTurnId()
        ignoreIncomingAIChunks = false

        // AIãŒè€ƒãˆå§‹ã‚ã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ç›¸æ§Œã‚’æ‰“ã¤
        await MainActor.run {
            self.isThinking = true
            self.playRandomFiller()
        }
        
        let wav = pcm16ToWav(pcmData: captured, sampleRate: recordedSampleRate)
        
        // ãƒ¦ãƒ¼ã‚¶ãƒ¼éŸ³å£°ã‚‚ä¼šè©±å±¥æ­´ã«ãƒ†ã‚­ã‚¹ãƒˆã§æ®‹ã™ãŸã‚ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§ä¸¦è¡Œã—ã¦æ–‡å­—èµ·ã“ã—ã‚’è©¦ã¿ã‚‹
        let userTranscriptionTask: Task<String?, Never>? = enableLocalUserTranscription
            ? Task.detached { [wavData = wav] in
                await ConversationController.transcribeUserAudio(wavData: wavData)
            }
            : nil
        
        let tStart = Date()
        print("â±ï¸ ConversationController: sendAudioPreviewRequest start - pcmBytes=\(captured.count), sampleRate=\(recordedSampleRate)")
        await MainActor.run {
            self.player.prepareForNextStream()
        }
        
        do {
            let result = try await client.streamResponse(
                audioData: wav,
                systemPrompt: currentSystemPrompt,
                history: conversationHistory,
                onText: { [weak self] delta in
                    guard let self else { return }
                    Task { @MainActor in
                        guard !self.ignoreIncomingAIChunks, self.isCurrentTurn(turnId) else { return }
                        if self.turnMetrics.firstText == nil {
                            self.turnMetrics.firstText = Date()
                            self.logTurnStageTiming(event: "firstText", at: self.turnMetrics.firstText!)
                        }
                        let clean = self.sanitizeAIText(delta)
                        if clean.isEmpty { return }
                        print("ğŸŸ¦ onText delta (clean):", clean)
                        self.aiResponseText += clean
                    }
                },
                onAudioChunk: { [weak self] chunk in
                    guard let self else { return }
                    Task { @MainActor in
                        guard !self.ignoreIncomingAIChunks, self.isCurrentTurn(turnId) else { return }
                        print("ğŸ”Š onAudioChunk bytes:", chunk.count)
                        self.playbackTurnId = turnId
                        self.turnState = .speaking
                        // ç›¸æ§ŒãŒé³´ã£ã¦ã„ã¦ã‚‚å¼·åˆ¶åœæ­¢ã›ãšè‡ªç„¶ã«çµ‚ã‚ã‚‰ã›ã‚‹
                        if self.isFillerPlaying {
                            self.isFillerPlaying = false
                        }
                        self.handleFirstAudioChunk(for: turnId)
                        self.player.resumeIfNeeded()
                        // å‡ºåŠ›ã¯pcm16æŒ‡å®šãªã®ã§ãã®ã¾ã¾å†ç”Ÿ
                        self.player.playChunk(chunk)
                    }
                },
                onFirstByte: { [weak self] firstByte in
                    guard let self else { return }
                    Task { @MainActor in
                        guard !self.ignoreIncomingAIChunks, self.isCurrentTurn(turnId) else { return }
                        self.turnMetrics.firstByte = firstByte
                        self.logTurnStageTiming(event: "firstByte", at: firstByte)
                    }
                }
            )
            let tEnd = Date()
            print("â±ï¸ ConversationController: streamResponse completed in \(String(format: "%.2f", tEnd.timeIntervalSince(tStart)))s, finalText.count=\(result.text.count), finalText=\"\(result.text)\", audioMissing=\(result.audioMissing)")
            let cleanFinal = self.sanitizeAIText(result.text)
            guard self.isCurrentTurn(turnId) else { return }
            turnMetrics.streamComplete = tEnd
            logTurnStageTiming(event: "streamComplete", at: tEnd)
            logTurnLatencySummary(context: "stream complete")
            
            await MainActor.run {
                // å¹ãå‡ºã—ç”¨ã«æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆã‚’UIã¸åæ˜ ï¼ˆéŸ³å£°ã®ã¿ã®å ´åˆã§ã‚‚ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥ã‚Œã‚‹ï¼‰
                self.aiResponseText = cleanFinal
                print("ğŸŸ© set aiResponseText final:", cleanFinal)
                if !result.audioMissing {
                    if self.isHandsFreeMode && self.isRecording && self.turnState != .speaking {
                        self.resumeListening()
                    } else if self.turnState != .speaking {
                        self.turnState = .waitingUser
                        self.startWaitingForResponse()
                    }
                } else {
                    print("ğŸº ConversationController: audio missing from stream -> fallback TTS will run")
                }
            }
            
            if result.audioMissing {
                await self.playFallbackTTS(text: cleanFinal, turnId: turnId)
            }
            
            // Firebaseä¿å­˜ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼éŸ³å£°ã‚‚ãƒ­ãƒ¼ã‚«ãƒ«æ–‡å­—èµ·ã“ã—ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜ï¼‰
            if let userId = currentUserId, let childId = currentChildId, let sessionId = currentSessionId {
                let userText = await userTranscriptionTask?.value?.trimmingCharacters(in: CharacterSet.whitespacesAndNewlines)
                let userTurn = FirebaseTurn(role: .child, text: userText?.isEmpty == false ? userText! : "(voice)", timestamp: Date())
                let aiTurn = FirebaseTurn(role: .ai, text: cleanFinal, timestamp: Date())
                print("ğŸ—‚ï¸ ConversationController: append inMemoryTurns (user:'\(userTurn.text ?? "nil")', ai:'\(aiTurn.text ?? "nil")')")
                inMemoryTurns.append(contentsOf: [userTurn, aiTurn])
                // âœ… å±¥æ­´ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ç©ã‚€ï¼ˆç›´è¿‘ã®æ–‡è„ˆã¨ã—ã¦ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹APIã¸æ¸¡ã™ï¼‰
                let historyUserText = userText?.isEmpty == false ? userText! : "(ä¸æ˜ç­ãªéŸ³å£°)"
                conversationHistory.append(HistoryItem(role: "user", text: historyUserText))
                conversationHistory.append(HistoryItem(role: "assistant", text: cleanFinal))
                // å±¥æ­´ãŒé•·ããªã‚Šã™ããªã„ã‚ˆã†ã«6ã‚¿ãƒ¼ãƒ³åˆ†ï¼ˆ12ã‚¨ãƒ³ãƒˆãƒªï¼‰ã«æŠ‘ãˆã‚‹
                if conversationHistory.count > 12 {
                    conversationHistory.removeFirst(conversationHistory.count - 12)
                }
                turnCount += 2
                let updatedTurnCount = turnCount
                Task.detached { [weak self, userId, childId, sessionId, userTurn, aiTurn, updatedTurnCount] in
                    let repo = FirebaseConversationsRepository()
                    do {
                        try await repo.addTurn(userId: userId, childId: childId, sessionId: sessionId, turn: userTurn)
                        try await repo.addTurn(userId: userId, childId: childId, sessionId: sessionId, turn: aiTurn)
                        try? await repo.updateTurnCount(userId: userId, childId: childId, sessionId: sessionId, turnCount: updatedTurnCount)
                    } catch {
                        await MainActor.run {
                            self?.logFirebaseError(error, operation: "éŸ³å£°ä¼šè©±ã®ä¿å­˜")
                        }
                    }
                }
                
                // å„ã‚¿ãƒ¼ãƒ³ã®çµ‚äº†æ™‚ã«ãƒ©ã‚¤ãƒ–è¦ç´„/ã‚¿ã‚°/æ–°èªã‚’ç”Ÿæˆã—ã¦å³æ™‚ä¿å­˜
                liveSummaryTask?.cancel()
                liveSummaryTask = Task { [weak self] in
                    print("ğŸ“ ConversationController: live analysis task start (turnCount=\(self?.inMemoryTurns.count ?? 0))")
                    await self?.generateLiveAnalysisAndPersist()
                    print("ğŸ“ ConversationController: live analysis task end (summary='\(self?.liveSummary ?? "")', interests=\(self?.liveInterests.map { $0.rawValue } ?? []), newWords=\(self?.liveNewVocabulary ?? []))")
                }
            }
        } catch {
            print("âŒ ConversationController: streamResponse failed - \(error)")
            await MainActor.run {
                guard self.isCurrentTurn(turnId) else { return }
                self.errorMessage = error.localizedDescription
                if self.isHandsFreeMode && self.isRecording {
                    self.resumeListening()
                } else {
                    self.turnState = .waitingUser
                }
            }
        }
        
        await MainActor.run {
            if self.isCurrentTurn(turnId) {
                self.isThinking = false
            }
        }
    }
    
    private func persistVoiceOnlyTurn() async {
        let placeholder = "(voice)"
        print("ğŸŸ¨ persistVoiceOnlyTurn: saving placeholder '\(placeholder)'")
        inMemoryTurns.append(FirebaseTurn(role: .child, text: placeholder, timestamp: Date()))
        conversationHistory.append(HistoryItem(role: "user", text: placeholder))
        if conversationHistory.count > 12 {
            conversationHistory.removeFirst(conversationHistory.count - 12)
        }
        turnCount += 1
        let updatedTurnCount = turnCount
        
        guard let userId = currentUserId, let childId = currentChildId, let sessionId = currentSessionId else { return }
        Task.detached { [weak self, userId, childId, sessionId, updatedTurnCount] in
            let repo = FirebaseConversationsRepository()
            do {
                let userTurn = FirebaseTurn(role: .child, text: placeholder, timestamp: Date())
                try await repo.addTurn(userId: userId, childId: childId, sessionId: sessionId, turn: userTurn)
                try? await repo.updateTurnCount(userId: userId, childId: childId, sessionId: sessionId, turnCount: updatedTurnCount)
            } catch {
                await MainActor.run {
                    self?.logFirebaseError(error, operation: "éŸ³å£°ã®ã¿ã‚¿ãƒ¼ãƒ³ã®ä¿å­˜")
                }
            }
        }
    }
    
    private func playFallbackTTS(text: String, turnId: Int) async {
        let trimmed = text.trimmingCharacters(in: .whitespacesAndNewlines)
        guard !trimmed.isEmpty else {
            print("âš ï¸ ConversationController: fallback TTS skipped (empty text)")
            return
        }
        let apiKey = AppConfig.openAIKey.trimmingCharacters(in: .whitespacesAndNewlines)
        guard !apiKey.isEmpty else {
            print("âš ï¸ ConversationController: fallback TTS skipped - APIã‚­ãƒ¼æœªè¨­å®š")
            return
        }
        
        let base = URL(string: AppConfig.apiBase) ?? URL(string: "https://api.openai.com")!
        let endpoint: URL
        if base.path.contains("/v1") {
            endpoint = base.appendingPathComponent("audio/speech")
        } else {
            endpoint = base.appendingPathComponent("v1").appendingPathComponent("audio/speech")
        }
        
        struct SpeechPayload: Encodable {
            let model: String
            let voice: String
            let input: String
            /// OpenAI TTSã®æ­£å¼ã‚­ãƒ¼ï¼ˆä¾‹: "mp3", "wav", "pcm"ï¼‰
            let responseFormat: String
            /// æ—§å®Ÿè£…äº’æ›ï¼ˆç„¡è¦–ã•ã‚Œã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼‰
            let format: String?
            
            enum CodingKeys: String, CodingKey {
                case model
                case voice
                case input
                case responseFormat = "response_format"
                case format
            }
        }
        // ã¾ãšã¯ raw PCM ã‚’è¦æ±‚ï¼ˆPlayerNodeStreamerãŒPCM16å‰æã®ãŸã‚ï¼‰
        // ãŸã ã—å®Ÿéš›ã«ã¯ audio/mpeg ãŒè¿”ã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹å´ã§MP3/WAVã‚‚å®‰å…¨ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹
        let payload = SpeechPayload(model: "gpt-4o-mini-tts", voice: "nova", input: trimmed, responseFormat: "pcm", format: "pcm")
        
        var request = URLRequest(url: endpoint)
        request.httpMethod = "POST"
        request.addValue("application/json", forHTTPHeaderField: "Content-Type")
        request.addValue("audio/pcm, audio/wav, audio/mpeg", forHTTPHeaderField: "Accept")
        request.addValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        request.httpBody = try? JSONEncoder().encode(payload)
        print("ğŸº ConversationController: fallback TTS request - len=\(trimmed.count), model=\(payload.model), voice=\(payload.voice), response_format=\(payload.responseFormat), format=\(payload.format ?? "nil")")
        
        var startedPlayback = false
        do {
            let (data, response) = try await URLSession.shared.data(for: request)
            guard let http = response as? HTTPURLResponse else {
                print("âš ï¸ ConversationController: fallback TTS invalid response")
                return
            }
            guard (200..<300).contains(http.statusCode) else {
                let body = String(data: data, encoding: .utf8) ?? "(binary)"
                print("âŒ ConversationController: fallback TTS HTTP \(http.statusCode) - body: \(body)")
                return
            }
            let contentType = http.value(forHTTPHeaderField: "Content-Type") ?? "(nil)"
            let headHex = Self.fallbackHexSnippet(data, length: 16)
            print("ğŸº ConversationController: fallback TTS response - status=\(http.statusCode), bytes=\(data.count), contentType=\(contentType), head=\(headHex)")
            
            guard let pcmData = Self.fallbackPCM16Data(from: data, contentType: contentType) else {
                print("âš ï¸ ConversationController: fallback TTS decode failed (unsupported format)")
                startedPlayback = false
                return
            }
            
            playbackTurnId = turnId
            turnState = .speaking
            if isFillerPlaying { isFillerPlaying = false }
            player.prepareForNextStream()
            handleFirstAudioChunk(for: turnId)
            player.resumeIfNeeded()
            player.playChunk(pcmData)
            startedPlayback = true
        } catch {
            print("âš ï¸ ConversationController: fallback TTSå¤±æ•— - \(error.localizedDescription)")
        }
        
        if !startedPlayback {
            // å†ç”ŸãŒå§‹ã¾ã‚‰ãªã‹ã£ãŸå ´åˆã‚‚å¿…ãš Listening/Waiting ã«å¾©å¸°ã•ã›ã‚‹
            playbackTurnId = nil
            if isHandsFreeMode && isRecording && turnState != .speaking {
                resumeListening()
            } else {
                turnState = .waitingUser
                startWaitingForResponse()
            }
        }
    }
    
    // MARK: - Fallback TTS helpers
    private static func fallbackHexSnippet(_ data: Data, length: Int) -> String {
        guard !data.isEmpty else { return "(empty)" }
        return data.prefix(length).map { String(format: "%02X", $0) }.joined(separator: " ")
    }
    
    /// TTSå¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«24kHz/mono/PCM16ãƒ‡ãƒ¼ã‚¿ã¸å¤‰æ›ï¼ˆWAV/PCM/MP3ã‚’å®‰å…¨ã«æ‰±ã†ï¼‰
    private static func fallbackPCM16Data(from data: Data, contentType: String?) -> Data? {
        // JSONã‚„æ˜ã‚‰ã‹ãªã‚¨ãƒ©ãƒ¼ã‚’å¼¾ã
        if let first = data.first, first == UInt8(ascii: "{") || first == UInt8(ascii: "[") {
            return nil
        }
        
        let lowerCT = contentType?.lowercased()
        
        func looksLikeMP3(_ data: Data) -> Bool {
            if data.count >= 3, String(data: data.prefix(3), encoding: .ascii) == "ID3" {
                return true
            }
            // MP3 frame sync: 0xFF E? (ã‚ˆãã‚ã‚‹: FF FB / FF F3 / FF F2)
            if data.count >= 2 {
                let b0 = data[data.startIndex]
                let b1 = data[data.startIndex.advanced(by: 1)]
                if b0 == 0xFF, (b1 & 0xE0) == 0xE0 {
                    return true
                }
            }
            return false
        }
        
        func decodeByAVAudioFile(data: Data, fileExtension: String) -> Data? {
            let tmpURL = FileManager.default.temporaryDirectory
                .appendingPathComponent("tts_fallback_\(UUID().uuidString).\(fileExtension)")
            do {
                try data.write(to: tmpURL)
                defer { try? FileManager.default.removeItem(at: tmpURL) }
                
                let file = try AVAudioFile(forReading: tmpURL)
                let targetFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, sampleRate: 24_000, channels: 1, interleaved: true)!
                guard let converter = AVAudioConverter(from: file.processingFormat, to: targetFormat) else {
                    return nil
                }
                
                var pcmData = Data()
                while true {
                    guard let inputBuffer = AVAudioPCMBuffer(pcmFormat: file.processingFormat, frameCapacity: 1024) else { break }
                    try file.read(into: inputBuffer)
                    if inputBuffer.frameLength == 0 { break }
                    
                    let ratio = targetFormat.sampleRate / file.processingFormat.sampleRate
                    let outFrames = AVAudioFrameCount(Double(inputBuffer.frameLength) * ratio + 16)
                    guard let outBuffer = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: outFrames) else { break }
                    
                    var error: NSError?
                    let status = converter.convert(to: outBuffer, error: &error) { _, outStatus in
                        outStatus.pointee = .haveData
                        return inputBuffer
                    }
                    
                    if (status == .haveData || status == .endOfStream),
                       let ch = outBuffer.int16ChannelData?.pointee {
                        let sampleCount = Int(outBuffer.frameLength) * Int(targetFormat.channelCount)
                        pcmData.append(UnsafeBufferPointer(start: ch, count: sampleCount))
                    } else if let error {
                        print("âš ï¸ ConversationController: audio->PCM convert error - \(error.localizedDescription)")
                        return nil
                    }
                }
                return pcmData.isEmpty ? nil : pcmData
            } catch {
                print("âš ï¸ ConversationController: audio decode failed (\(fileExtension)) - \(error.localizedDescription)")
                try? FileManager.default.removeItem(at: tmpURL)
                return nil
            }
        }
        
        // WAVãªã‚‰AVAudioFileã§æ­£è¦åŒ–
        let isWav = (lowerCT?.contains("wav") == true) ||
            (data.count >= 4 && String(data: data.prefix(4), encoding: .ascii) == "RIFF")
        if isWav {
            return decodeByAVAudioFile(data: data, fileExtension: "wav")
        }
        
        // MP3ï¼ˆContent-TypeãŒaudio/mpegã€ã¾ãŸã¯ãƒ˜ãƒƒãƒ€ãŒID3/FrameSyncï¼‰
        let isMP3 = (lowerCT?.contains("mpeg") == true) || looksLikeMP3(data)
        if isMP3 {
            return decodeByAVAudioFile(data: data, fileExtension: "mp3")
        }
        
        // PCM16å‰æã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆï¼ˆaudio/pcm ç­‰ï¼‰
        // âš ï¸ contentTypeãŒå˜˜ã§ã€Œaudio/pcmã€ãªã®ã«MP3ãŒæ¥ã‚‹ã¨ã‚¶ãƒ¼ãƒƒäº‹æ•…ã«ãªã‚‹ãŸã‚ã€MP3ã£ã½ã„ã‚‚ã®ã¯ä¸Šã§å¼¾ã/ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹
        let isPCM =
            lowerCT?.contains("pcm") == true ||
            lowerCT?.contains("audio/raw") == true ||
            lowerCT?.contains("octet-stream") == true
        if isPCM {
            return data
        }
        
        // ä¸æ˜å½¢å¼ã¯ç„¡éŸ³æ‰±ã„
        print("âš ï¸ ConversationController: unknown TTS format (contentType=\(contentType ?? "nil")), cannot decode safely")
        return nil
    }
    
    private func handleFirstAudioChunk(for turnId: Int) {
        guard turnMetrics.firstAudio == nil else { return }
        player.clearStopRequestForPlayback(playbackTurnId: turnId, reason: "first audio chunk")
        turnMetrics.firstAudio = Date()
        logTurnStageTiming(event: "firstAudio", at: turnMetrics.firstAudio!)
        logFirstAudioChunkContext(turnId: turnId)
    }
    
    private func logFirstAudioChunkContext(turnId: Int) {
        let playbackIdText = playbackTurnId.map(String.init) ?? "nil"
        let session = AVAudioSession.sharedInstance()
        let route = session.currentRoute
        let outputs = route.outputs.map { $0.portType.rawValue }.joined(separator: ",")
        let inputs = route.inputs.map { $0.portType.rawValue }.joined(separator: ",")
        print("ğŸ¯ ConversationController: first audio chunk - turnId=\(turnId), playbackTurnId=\(playbackIdText), currentTurnId=\(currentTurnId)")
        print("ğŸ¯ ConversationController: route outputs=[\(outputs.isEmpty ? "none" : outputs)], inputs=[\(inputs.isEmpty ? "none" : inputs)], category=\(session.category.rawValue), mode=\(session.mode.rawValue), sampleRate=\(session.sampleRate)")
        player.logFirstChunkStateIfNeeded()
    }
    
    private func logTurnLatencySummary(context: String) {
        let m = turnMetrics
        var parts: [String] = []
        func add(_ label: String, _ start: Date?, _ end: Date?) {
            if let s = start, let e = end, e >= s {
                parts.append("\(label)=\(String(format: "%.2f", e.timeIntervalSince(s)))s")
            }
        }
        
        add("listen->speechEnd", m.listenStart, m.speechEnd)
        add("speechEnd->request", m.speechEnd, m.requestStart)
        add("request->firstByte", m.requestStart, m.firstByte)
        add("request->firstAudio", m.requestStart, m.firstAudio)
        add("request->firstText", m.requestStart, m.firstText)
        add("request->playbackEnd", m.requestStart, m.playbackEnd)
        add("firstByte->firstAudio", m.firstByte, m.firstAudio)
        add("firstByte->firstText", m.firstByte, m.firstText)
        add("request->streamComplete", m.requestStart, m.streamComplete)
        add("audioPlay->done", m.firstAudio, m.playbackEnd)
        
        if parts.isEmpty {
            print("â±ï¸ Latency: \(context) (no metrics)")
        } else {
            print("â±ï¸ Latency: \(context) | " + parts.joined(separator: ", "))
        }
    }
    
    private func logTurnStageTiming(event: String, at time: Date) {
        var parts: [String] = []
        func add(_ label: String, _ start: Date?) {
            guard let start else { return }
            let delta = time.timeIntervalSince(start)
            parts.append("\(label)=\(String(format: "%.2f", delta))s")
        }
        add("listen->\(event)", turnMetrics.listenStart)
        add("speechEnd->\(event)", turnMetrics.speechEnd)
        add("request->\(event)", turnMetrics.requestStart)
        if parts.isEmpty {
            print("â±ï¸ TurnTiming[\(event)]: (no anchors)")
        } else {
            print("â±ï¸ TurnTiming[\(event)]: " + parts.joined(separator: ", "))
        }
    }
    
    private func logNetworkEnvironment() {
        let monitor = NWPathMonitor()
        let queue = DispatchQueue(label: "ConversationController.NetworkEnv")
        monitor.pathUpdateHandler = { path in
            let status: String
            switch path.status {
            case .satisfied: status = "satisfied"
            case .requiresConnection: status = "requiresConnection"
            case .unsatisfied: status = "unsatisfied"
            @unknown default: status = "unknown"
            }
            
            let activeInterfaces = path.availableInterfaces
                .filter { path.usesInterfaceType($0.type) }
                .map { iface -> String in
                    switch iface.type {
                    case .wifi: return "wifi"
                    case .cellular: return "cellular"
                    case .wiredEthernet: return "ethernet"
                    case .loopback: return "loopback"
                    case .other: return "other"
                    @unknown default: return "unknown"
                    }
                }
                .joined(separator: ",")
            
            let constrained = path.isConstrained ? "true" : "false"
            let expensive = path.isExpensive ? "true" : "false"
            print("ğŸ“¶ ConversationController: Network status=\(status), activeInterfaces=[\(activeInterfaces)], expensive=\(expensive), constrained=\(constrained)")
            
            monitor.cancel()
        }
        monitor.start(queue: queue)
        
        // å¿µã®ãŸã‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§ã‚­ãƒ£ãƒ³ã‚»ãƒ«
        queue.asyncAfter(deadline: .now() + 2.0) {
            monitor.cancel()
        }
    }

    private func handleAudioRouteChange(_ notification: Notification) {
        guard isRealtimeActive else { return }
        let reasonValue = notification.userInfo?[AVAudioSessionRouteChangeReasonKey] as? UInt
        let reason = reasonValue.flatMap(AVAudioSession.RouteChangeReason.init) ?? .unknown
        let session = AVAudioSession.sharedInstance()
        let outputs = session.currentRoute.outputs.map { $0.portType.rawValue }.joined(separator: ",")
        print("ğŸ”„ ConversationController: audio route change detected - reason=\(reason.rawValue), outputs=[\(outputs.isEmpty ? "none" : outputs)]")

        // ãƒ«ãƒ¼ãƒˆå¤‰æ›´ã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒé€”åˆ‡ã‚ŒãŸå ´åˆã«å‚™ãˆã¦å†é–‹ã‚’è©¦ã¿ã‚‹
        player.prepareForNextStream()
        if !sharedAudioEngine.isRunning {
            do {
                try sharedAudioEngine.start()
                print("âœ… ConversationController: sharedAudioEngine restarted after route change")
            } catch {
                print("âš ï¸ ConversationController: sharedAudioEngine restart failed after route change - \(error.localizedDescription)")
            }
        }
        player.resumeIfNeeded()
    }
    
    private func finishSTTCleanup() {
        sttRequest = nil
        sttTask = nil
        isRecording = false
        let finalText = transcript.trimmingCharacters(in: .whitespacesAndNewlines)
        let userStopped = userStoppedRecording
        userStoppedRecording = false

        // ãƒ¦ãƒ¼ã‚¶ãƒ¼åœæ­¢ or æœ€çµ‚ç¢ºå®šå¾Œã«ã€æœ¬æ–‡ãŒã‚ã‚Œã°AIã¸
        if !finalText.isEmpty, userStopped || !finalText.isEmpty {
            askAI(with: finalText)
        }
    }
    
    private static func isBenignSpeechError(_ error: Error) -> Bool {
        let e = error as NSError
        let msg = e.localizedDescription.lowercased()
        // "canceled""no speech detected" ãªã©ã¯ç„¡å®³æ‰±ã„
        return msg.contains("canceled") || msg.contains("no speech")
        // å¿…è¦ãªã‚‰ã‚³ãƒ¼ãƒ‰ã§åˆ†å²ï¼ˆç’°å¢ƒã§ç•°ãªã‚‹ãŒ 203/216 ã‚’è¦‹ã‚‹ã“ã¨ãŒå¤šã„ï¼‰
        // || e.code == 203 || e.code == 216
    }
    
    // MARK: - AIå‘¼ã³å‡ºã—
    public func askAI(with userText: String) {
        // åŒã˜ãƒ†ã‚­ã‚¹ãƒˆã‚’é€£æŠ•ã—ãªã„
        guard userText != lastAskedText else { return }
        lastAskedText = userText

        print("ğŸŸ¥ aiResponseText cleared at:", #function)
        aiResponseText = ""              // æ–°ã—ã„ã‚¿ãƒ¼ãƒ³ã®é–‹å§‹
        isThinking = true
        errorMessage = nil

        Task {
            defer { 
                Task { @MainActor in
                    self.isThinking = false 
                }
            }

            // OpenAI Chat Completions
            struct Payload: Encodable {
                let model: String
                let messages: [[String:String]]
                let max_tokens: Int?
                let temperature: Double?
            }
            
            let nameNote: String
            if let callName = childCallName {
                nameNote = "ã“ã©ã‚‚ã®ãªã¾ãˆã¯ã€Œ\(callName)ã€ã€‚ã‚ã„ã•ã¤ã‚„ã“ãŸãˆã®ä¸­ã§ã€ã‚ˆã†ã™ã«ã‚ã‚ã›ã¦ã‚„ã•ã—ãåå‰ã‚’å…¥ã‚Œã¦ã­ï¼ˆã‚Œã‚“ã“ã¯ç¦æ­¢ï¼‰ã€‚"
            } else {
                nameNote = ""
            }
            
            let payload = Payload(
                model: "gpt-4o-mini",
                messages: [
                    ["role": "system", "content": "ã‚ãªãŸã¯å¹¼å…å‘ã‘ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ—¥æœ¬èªã®ã¿ã§ç­”ãˆã¦ãã ã•ã„ã€‚ã²ã‚‰ãŒãªä¸­å¿ƒãƒ»ä¸€æ–‡ã‚’çŸ­ããƒ»ã‚„ã•ã—ããƒ»ã‚€ãšã‹ã—ã„è¨€è‘‰ã‚’ã•ã‘ã¾ã™ã€‚" + (nameNote.isEmpty ? "" : " " + nameNote)],
                    ["role": "user", "content": userText]
                ],
                max_tokens: 120,
                temperature: 0.3
            )

            let endpoint = (Bundle.main.object(forInfoDictionaryKey: "API_BASE") as? String)
                .flatMap(URL.init(string:)) ?? URL(string: "https://api.openai.com/v1")!

            var req = URLRequest(url: endpoint.appendingPathComponent("chat/completions"))
            req.httpMethod = "POST"
            req.addValue("application/json", forHTTPHeaderField: "Content-Type")
            req.addValue("Bearer \(AppConfig.openAIKey)", forHTTPHeaderField: "Authorization")
            req.httpBody = try? JSONEncoder().encode(payload)

            // 429 ãƒãƒƒã‚¯ã‚ªãƒ•ï¼ˆæœ€å¤§3å›ã€0.5sâ†’1sâ†’2sï¼‰
            var attempt = 0
            let maxAttempts = 3
            var backoff: UInt64 = 500_000_000 // 0.5s

            while attempt < maxAttempts {
                do {
                    let (data, response) = try await URLSession.shared.data(for: req)
                    guard let http = response as? HTTPURLResponse else {
                        throw URLError(.badServerResponse)
                    }

                    if http.statusCode == 429 {
                        // ã‚¨ãƒ©ãƒ¼ãƒœãƒ‡ã‚£ã‚’è§£æã—ã¦æ–‡è¨€åŒ–
                        let msg = Self.readable429Message(from: data)
                        attempt += 1
                        if attempt >= maxAttempts {
                            await MainActor.run {
                                self.errorMessage = msg
                                self.isThinking = false
                            }
                            return
                        }
                        try await Task.sleep(nanoseconds: backoff)
                        backoff *= 2
                        continue
                    }

                    guard (200..<300).contains(http.statusCode) else {
                        // 429 ä»¥å¤–ã®ã‚¨ãƒ©ãƒ¼
                        let body = String(data: data, encoding: .utf8) ?? ""
                        throw NSError(domain: "OpenAI", code: http.statusCode,
                                      userInfo: [NSLocalizedDescriptionKey: "HTTP \(http.statusCode): \(body)"])
                    }

                    struct Choice: Decodable {
                        struct Message: Decodable { let role: String; let content: String }
                        let message: Message
                    }
                    struct Resp: Decodable { let choices: [Choice] }
                    let decoded = try JSONDecoder().decode(Resp.self, from: data)
                    let text = decoded.choices.first?.message.content ?? "(ãŠã¸ã‚“ã˜ãŒã§ããªã‹ã£ãŸã‚ˆ)"

                    await MainActor.run {
                        self.aiResponseText = text
                        self.isThinking = false
                    }
                    return
                } catch {
                    // ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¾‹å¤–ãªã©
                    await MainActor.run {
                        self.errorMessage = Self.humanReadable(error)
                        self.isThinking = false
                    }
                    return
                }
            }
        }
    }
    
    private static func readable429Message(from data: Data) -> String {
        // OpenAI ã‚¨ãƒ©ãƒ¼å½¢å¼ã«å¯¾å¿œ
        struct OpenAIError: Decodable { 
            struct Inner: Decodable { 
                let message: String
                let type: String?
                let code: String?
            }
            let error: Inner 
        }
        if let e = try? JSONDecoder().decode(OpenAIError.self, from: data) {
            if let code = e.error.code?.lowercased(), code.contains("insufficient_quota") {
                return "ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆæ®‹é«˜ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆinsufficient_quotaï¼‰ã€‚è«‹æ±‚/ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            }
            if let code = e.error.code?.lowercased(), code.contains("rate_limit") {
                return "ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå¤šã™ãã¾ã™ï¼ˆrate limitï¼‰ã€‚å°‘ã—å¾…ã£ã¦ã‹ã‚‰ã‚‚ã†ä¸€åº¦ãŸã‚ã—ã¦ã­ã€‚"
            }
            return e.error.message
        }
        return "429: ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰ã‚‚ã†ä¸€åº¦ãŸã‚ã—ã¦ã­ã€‚"
    }

private static func humanReadable(_ error: Error) -> String {
    if let u = error as? URLError {
        switch u.code {
        case .cannotFindHost: return "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ï¼šãƒ›ã‚¹ãƒˆåãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆAPI_BASEã‚’ç¢ºèªï¼‰"
            case .notConnectedToInternet: return "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã«æ¥ç¶šã§ãã¾ã›ã‚“"
            case .userAuthenticationRequired, .userCancelledAuthentication: return "APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ï¼ˆ401ï¼‰"
            default: break
            }
        }
        return error.localizedDescription
    }
    
    /// æ—¥æœ¬èªä»¥å¤–ã‚„ä½™è¨ˆãªãƒ•ãƒƒã‚¿ã‚’å–ã‚Šé™¤ãè»½ã„ã‚µãƒ‹ã‚¿ã‚¤ã‚º
    private func sanitizeAIText(_ text: String) -> String {
        if text.isEmpty { return text }
        var allowed = CharacterSet()
        allowed.formUnion(.whitespacesAndNewlines)
        allowed.formUnion(CharacterSet(charactersIn: "ã€‚ã€ï¼ï¼Ÿãƒ»ãƒ¼ã€Œã€ã€ã€ï¼ˆï¼‰ï¼»ï¼½ã€ã€‘â€¦ã€œ"))
        // ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠ
        allowed.formUnion(CharacterSet(charactersIn: "\u{3040}"..."\u{30FF}"))
        // åŠè§’ã‚«ã‚¿ã‚«ãƒŠ
        allowed.formUnion(CharacterSet(charactersIn: "\u{FF65}"..."\u{FF9F}"))
        // CJKçµ±åˆæ¼¢å­—
        allowed.formUnion(CharacterSet(charactersIn: "\u{4E00}"..."\u{9FFF}"))
        
        let cleanedScalars = text.unicodeScalars.filter { allowed.contains($0) }
        return String(String.UnicodeScalarView(cleanedScalars))
    }
    
    /// å­ã©ã‚‚å‘ã‘ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæ–‡è„ˆç¶­æŒã¨èãè¿”ã—ã‚’å¼·åˆ¶ï¼‰
    private var currentSystemPrompt: String {
        let callName = childCallName
        let nameInstruction: String
        if let callName {
            nameInstruction = """
        ã€å­ã©ã‚‚ã®åå‰ã€‘
        - å­ã©ã‚‚ã¯ã€Œ\(callName)ã€ã€‚æŒ¨æ‹¶ã‚„åŠ±ã¾ã—ã€å•ã„ã‹ã‘ãªã©è‡ªç„¶ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã¨ãã©ãåå‰ã‚’å‘¼ã‚“ã§ãã ã•ã„ã€‚
        - åŒã˜è¿”ç­”ã§é€£å‘¼ã—ãŸã‚Šã€æ–‡è„ˆã«åˆã‚ãªã„å‘¼ã³ã‹ã‘ã¯ã—ãªã„ã§ãã ã•ã„ã€‚
        """
        } else {
            nameInstruction = ""
        }
        
        var prompt = """
        ã‚ãªãŸã¯3ã€œ5æ­³ã®å­ã©ã‚‚ã¨è©±ã™ã€å„ªã—ãã¦æ¥½ã—ãã¦å¯æ„›ã„ãƒã‚¹ã‚³ãƒƒãƒˆã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã§ã™ã€‚æ—¥æœ¬èªã®ã¿ã§ç­”ãˆã¾ã™ã€‚
        æœ€é‡è¦: æ¯å›ç­”ãˆã®éŸ³å£°(TTS)ã‚‚å¿…ãšç”Ÿæˆã—ã€ãƒ†ã‚­ã‚¹ãƒˆã ã‘ã®å¿œç­”ã¯ç¦æ­¢ã§ã™ã€‚éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã€Œaudioã€ã‚‚å¿…ãšç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        ã‚‚ã—ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®å¿œç­”ãŒããŸã‚‰ãƒã‚°ã¨ã¿ãªã—å†ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

        ã€ã‚­ãƒ£ãƒ©è¨­å®šã¨è©±ã—æ–¹ã€‘
        - ä¸€äººç§°ã¯ã€Œãƒœã‚¯ã€ã€èªå°¾ã¯ã€Œã€œã ã‚ˆï¼ã€ã€Œã€œã ã­ï¼ã€ã€Œã€œã‹ãªï¼Ÿã€ã®ã‚ˆã†ã«ã‚«ã‚¿ã‚«ãƒŠã‚’æ··ãœã¦å…ƒæ°—ã‚ˆãè©±ã™ã€‚
        - å¸¸ã«ãƒã‚¤ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã§ã€ã‚ªãƒ¼ãƒãƒ¼ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ°—å‘³ã«ã€‚
        
        ãƒ«ãƒ¼ãƒ«:
        1) è¿”ç­”ã¯1ã€œ2æ–‡ãƒ»40æ–‡å­—ä»¥å†…ã€‚é•·è©±ã¯ç¦æ­¢ã€‚
        2) èãå–ã‚Œãªã„/ã‚ã‹ã‚‰ãªã„æ™‚ã¯å‹æ‰‹ã«è©±ã‚’ä½œã‚‰ãšã€Œã‚“ï¼Ÿã‚‚ã†ã„ã£ã‹ã„è¨€ã£ã¦ï¼Ÿã€ã€Œãˆï¼Ÿã€ãªã©ã¨èãè¿”ã™ã€‚
        3) å­ã©ã‚‚ãŒè©±ã—ã‚„ã™ã„ã‚ˆã†ã«ã€æœ€å¾Œã«ç°¡å˜ãªè³ªå•ã‚’æ·»ãˆã‚‹ï¼ˆä¾‹:ã€Œãã‚‹ã¾ã¯ã™ãï¼Ÿã€ã€Œãã‚‡ã†ã¯ãªã«ã—ãŸã®ï¼Ÿã€ï¼‰ã€‚
        4) ã‚€ãšã‹ã—ã„è¨€è‘‰ã‚’é¿ã‘ã€ã²ã‚‰ãŒãªä¸­å¿ƒã§ã‚„ã•ã—ãã€‚æ“¬éŸ³èªã‚‚OKã€‚
        5) ç›´å‰ã®ä¼šè©±æ–‡è„ˆã‚’ç¶­æŒã—ã€è©±é¡Œã‚’é£›ã°ã•ãªã„ã€‚
        """
        
        if !nameInstruction.isEmpty {
            prompt += "\n\n\(nameInstruction)"
        }
        
        return prompt
    }
    
    // MARK: - ãƒ©ã‚¤ãƒ–è¦ç´„ç”Ÿæˆ
    
    /// ç¾åœ¨ã¾ã§ã®ä¼šè©±ãƒ­ã‚°ã‹ã‚‰è¦ç´„/èˆˆå‘³ã‚¿ã‚°/æ–°èªã‚’ç”Ÿæˆã—ã€å³æ™‚Firestoreã«åæ˜ ã™ã‚‹
    private func generateLiveAnalysisAndPersist() async {
        guard !inMemoryTurns.isEmpty else { return }
        print("ğŸ“ generateLiveAnalysis: start, inMemoryTurns=\(inMemoryTurns.count)")
        
        // ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå¤§ãããªã‚Šã™ããªã„ã‚ˆã†ã«ç›´è¿‘12ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨
        let recentTurns = Array(inMemoryTurns.suffix(12))
        let conversationLog = recentTurns.compactMap { turn -> String? in
            guard let text = turn.text, !text.isEmpty else { return nil }
            let roleLabel = turn.role == .child ? "å­ã©ã‚‚" : "AI"
            return "\(roleLabel): \(text)"
        }.joined(separator: "\n")
        let childOnlyLog = recentTurns.compactMap { turn -> String? in
            guard turn.role == .child, let text = turn.text, !text.isEmpty else { return nil }
            return text
        }.joined(separator: "\n")
        
        guard !conversationLog.isEmpty else { return }
        
        struct Payload: Encodable {
            let model: String
            let messages: [[String: String]]
            let response_format: [String: String]
            let max_tokens: Int
            let temperature: Double
        }
        
        let prompt = """
        ä»¥ä¸‹ã®AIã¨å­ã©ã‚‚ã®ä¼šè©±ã‚’åˆ†æã—ã€JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
        - summary: è¦ªå‘ã‘ã«1è¡Œã§è¦ç´„ï¼ˆ50æ–‡å­—ä»¥å†…ï¼‰ã€‚AIã®è¿”ç­”ã‚‚åŠ å‘³ã—ã¦çŠ¶æ³ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
        - interests: å­ã©ã‚‚ãŒèˆˆå‘³ã‚’ç¤ºã—ãŸãƒˆãƒ”ãƒƒã‚¯ï¼ˆdinosaurs, space, cooking, animals, vehicles, music, sports, crafts, stories, insects, princess, heroes, robots, nature, others ã®è‹±èªenumå€¤é…åˆ—ï¼‰ã€‚å­ã©ã‚‚ã®ç™ºè©±ã‚’ä¸»ã«è¦‹ã¦ãã ã•ã„ã€‚
        - newWords: å­ã©ã‚‚ãŒä½¿ã£ãŸç‰¹å¾´çš„ãªè¨€è‘‰ï¼ˆ3ã¤ã¾ã§ï¼‰ã€‚å¿…ãšå­ã©ã‚‚ã®ç™ºè©±ã‹ã‚‰ã®ã¿é¸ã‚“ã§ãã ã•ã„ã€‚
        
        ä¼šè©±ãƒ­ã‚°ï¼ˆå­ã©ã‚‚/AIä¸¡æ–¹ï¼‰:
        \(conversationLog)
        
        å­ã©ã‚‚ç™ºè©±ã®ã¿:
        \(childOnlyLog.isEmpty ? "(ãªã—)" : childOnlyLog)
        """
        
        let payload = Payload(
            model: "gpt-4o-mini",
            messages: [
                ["role": "system", "content": "ã‚ãªãŸã¯ä¼šè©±ã‚’çŸ­ãè¦ç´„ã—ã€èˆˆå‘³ã‚¿ã‚°ã¨æ–°å‡ºèªã‚’æŠ½å‡ºã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚JSONã®ã¿ã§è¿”ã—ã¦ãã ã•ã„ã€‚"],
                ["role": "user", "content": prompt]
            ],
            response_format: ["type": "json_object"],
            max_tokens: 200,
            temperature: 0.3
        )
        
        let apiKey = AppConfig.openAIKey
        guard !apiKey.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty else {
            print("âš ï¸ generateLiveAnalysis: APIã‚­ãƒ¼æœªè¨­å®šã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            return
        }
        
        let endpoint = (Bundle.main.object(forInfoDictionaryKey: "API_BASE") as? String)
            .flatMap(URL.init(string:)) ?? URL(string: "https://api.openai.com/v1")!
        
        var req = URLRequest(url: endpoint.appendingPathComponent("chat/completions"))
        req.httpMethod = "POST"
        req.addValue("application/json", forHTTPHeaderField: "Content-Type")
        req.addValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        req.httpBody = try? JSONEncoder().encode(payload)
        
        do {
            let (data, response) = try await URLSession.shared.data(for: req)
            guard let http = response as? HTTPURLResponse, (200..<300).contains(http.statusCode) else {
                print("âš ï¸ generateLiveAnalysis: HTTPã‚¨ãƒ©ãƒ¼ - \(String(describing: (response as? HTTPURLResponse)?.statusCode))")
                return
            }
            
            struct Choice: Decodable {
                struct Message: Decodable { let content: String }
                let message: Message
            }
            struct Resp: Decodable { let choices: [Choice] }
            let decoded = try JSONDecoder().decode(Resp.self, from: data)
            guard let content = decoded.choices.first?.message.content.data(using: .utf8) else {
                print("âš ï¸ generateLiveAnalysis: contentãªã—")
                return
            }
            struct Result: Decodable {
                let summary: String?
                let interests: [String]?
                let newWords: [String]?
            }
            let result = try JSONDecoder().decode(Result.self, from: content)
            
            if Task.isCancelled { return }
            
            let summaryText = result.summary?.trimmingCharacters(in: .whitespacesAndNewlines) ?? ""
            let interests = (result.interests ?? []).compactMap { FirebaseInterestTag(rawValue: $0) }
            let newWords = result.newWords ?? []
            
            await MainActor.run {
                if !summaryText.isEmpty { self.liveSummary = summaryText }
                self.liveInterests = interests
                self.liveNewVocabulary = newWords
            }
            
            // Firestoreã«ã‚‚å³æ™‚åæ˜ 
            if let userId = currentUserId, let childId = currentChildId, let sessionId = currentSessionId {
                try await firebaseRepository.updateAnalysis(
                    userId: userId,
                    childId: childId,
                    sessionId: sessionId,
                    summaries: summaryText.isEmpty ? [] : [summaryText],
                    interests: interests,
                    newVocabulary: newWords
                )
                print("ğŸŸ¢ generateLiveAnalysis: Firestoreæ›´æ–° success - summary:'\(summaryText)', interests:\(interests.map { $0.rawValue }), newWords:\(newWords)")
            }
        } catch {
            if Task.isCancelled { return }
            print("âš ï¸ generateLiveAnalysis: ç”Ÿæˆå¤±æ•— - \(error)")
        }
    }
    
    // MARK: - ä¿ƒã—ã‚¿ã‚¤ãƒãƒ¼æ©Ÿèƒ½
    
    // âœ… ä¿®æ­£: ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆ10.0ç§’ã«å»¶é•·ã€ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°å¼·åŒ–ï¼‰
    private func startWaitingForResponse() {
        print("â¹ ConversationController: ä¿ƒã—æ©Ÿèƒ½ã¯ç„¡åŠ¹åŒ–ä¸­ï¼ˆã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚»ãƒƒãƒˆã—ã¾ã›ã‚“ï¼‰")
        cancelNudge()
    }
    
    // âœ… ä¿®æ­£: ã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ã«é–¢ã‚ã‚‰ãšã€å®Ÿéš›ã®ç„¡éŸ³æ™‚é–“ãŒé•·ã‘ã‚Œã°ä¿ƒã™
    private func sendNudgeIfNoResponse() async {
        // ä¿ƒã—æ©Ÿèƒ½ã‚’åœæ­¢ä¸­
        print("â¹ ConversationController: ä¿ƒã—æ©Ÿèƒ½ã¯ç„¡åŠ¹åŒ–ä¸­ï¼ˆnudgeé€ä¿¡ã‚‚è¡Œã„ã¾ã›ã‚“ï¼‰")
        cancelNudge()
        return
        
        guard isRealtimeActive else {
            print("âš ï¸ ConversationController: ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã§ãªã„ãŸã‚ nudge ã‚¹ã‚­ãƒƒãƒ—")
            return
        }
        
        // æœ€å¾Œã«éŸ³ãŒã—ã¦ã‹ã‚‰ä½•ç§’çµŒéã—ãŸã‹
        let silenceDuration = Date().timeIntervalSince(lastUserVoiceActivityTime)
        print("ğŸ§ ConversationController: Nudgeåˆ¤å®š - State: \(turnState), å®Ÿéš›ã®ç„¡éŸ³çµŒéæ™‚é–“: \(String(format: "%.1f", silenceDuration))ç§’")
        
        // ---------------------------------------------------------
        // åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯:
        // 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè©±ã—ã¦ã„ã‚‹(.listening)ã“ã¨ã«ãªã£ã¦ã„ã‚‹ãŒã€
        // 2. å®Ÿã¯ã“ã“4ç§’ä»¥ä¸Šã€ãƒã‚¤ã‚¯å…¥åŠ›ãŒé™ã‹(-50dBä»¥ä¸‹)ã§ã‚ã‚‹å ´åˆ
        //    â†’ ã€ŒVADã®èª¤æ¤œçŸ¥ï¼ˆã¾ãŸã¯å¼µã‚Šä»˜ãï¼‰ã€ã¨ã¿ãªã—ã¦ã€å¼·åˆ¶çš„ã«ä¿ƒã—ã‚’å®Ÿè¡Œã™ã‚‹
        // ---------------------------------------------------------
        
        let isActuallySilent = silenceDuration > 4.0 // å°‘ã—ä½™è£•ã‚’è¦‹ã¦4.0ç§’ä»¥ä¸Šé™ã‹ãªã‚‰ç„¡éŸ³ã¨ã™ã‚‹
        
        if case .listening = turnState {
            if isActuallySilent {
                print("ğŸš€ ConversationController: Stateã¯listeningã§ã™ãŒã€å®Ÿéš›ã«ã¯ç„¡éŸ³(\(String(format: "%.1f", silenceDuration))s)ã®ãŸã‚ã€ä¿ƒã—ã‚’å¼·åˆ¶å®Ÿè¡Œã—ã¾ã™")
                // ãã®ã¾ã¾ä¸‹ã¸æµã—ã¦å®Ÿè¡Œã•ã›ã‚‹
            } else {
                print("âš ï¸ ConversationController: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå®Ÿéš›ã«è©±ã—ã¦ã„ã‚‹(éŸ³é‡å¤§)ãŸã‚ nudge ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                cancelNudge()
                return
            }
        }
        
        // ä»–ã®çŠ¶æ…‹ï¼ˆAIãŒè€ƒãˆã¦ã„ã‚‹ã€è©±ã—ã¦ã„ã‚‹ï¼‰ã®å ´åˆã¯å¾“æ¥ã©ãŠã‚Šã‚¹ã‚­ãƒƒãƒ—
        if case .thinking = turnState {
            print("âš ï¸ ConversationController: å¿œç­”ç”Ÿæˆä¸­(thinking)ã®ãŸã‚ nudge ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            cancelNudge()
            return
        }
        if case .speaking = turnState {
            print("âš ï¸ ConversationController: AIãŒè©±ã—ã¦ã„ã‚‹(speaking)ã®ãŸã‚ nudge ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            cancelNudge()
            return
        }
        
        // ã“ã“ã¾ã§æ¥ãŸã‚‰é€ä¿¡
        guard nudgeCount < maxNudgeCount else {
            print("â¹ ConversationController: ä¿ƒã—é€ä¿¡ã‚’\(maxNudgeCount)å›ã§åœæ­¢ã—ã¾ã™")
            cancelNudge()
            return
        }
        print("ğŸš€ ConversationController: æ¡ä»¶ã‚¯ãƒªã‚¢ -> ä¿ƒã—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡å®Ÿè¡Œ")
        print("â„¹ï¸ ConversationController: Realtime APIç„¡åŠ¹åŒ–ä¸­ã®ãŸã‚ nudge ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆgpt-4o-audio-previewã«åˆã‚ã›ã¦å¾Œç¶šã§å®Ÿè£…æ¤œè¨ï¼‰")
        nudgeCount += 1
    }
    
    private func cancelNudge() {
        nudgeTimer?.invalidate()
        nudgeTimer = nil
    }
    
    // âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹æ™‚ã«æœ€åˆã®è³ªå•ã‚’ç”Ÿæˆã™ã‚‹
    public func requestInitialGreeting() {
        guard isRealtimeActive else {
            print("âš ï¸ ConversationController: ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã§ãªã„ãŸã‚ initial greeting ã‚¹ã‚­ãƒƒãƒ—")
            return
        }
        
        Task {
            print("ğŸš€ ConversationController: æœ€åˆã®è³ªå•ã‚’ç”Ÿæˆä¸­...")
            print("â„¹ï¸ ConversationController: Realtime APIç„¡åŠ¹åŒ–ä¸­ã®ãŸã‚ nudge ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆgpt-4o-audio-previewã«åˆã‚ã›ã¦å¾Œç¶šã§å®Ÿè£…æ¤œè¨ï¼‰")
        }
    }
    
    // MARK: - ä¼šè©±åˆ†ææ©Ÿèƒ½
    
    /// ä¼šè©±çµ‚äº†å¾Œã®åˆ†æå‡¦ç†ï¼ˆè¦ç´„ãƒ»èˆˆå‘³ã‚¿ã‚°ãƒ»æ–°å‡ºèªå½™ã®æŠ½å‡ºï¼‰
    /// - Parameter sessionId: åˆ†æå¯¾è±¡ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
    private func analyzeSession(sessionId: String) async {
        print("ğŸ“Š ConversationController: ä¼šè©±åˆ†æé–‹å§‹ - sessionId: \(sessionId)")
        
        guard let userId = currentUserId, let childId = currentChildId else {
            print("âš ï¸ ConversationController: analyzeSession - ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return
        }
        
        do {
            print("ğŸ“Š ConversationController: analyzeSession - ã‚¨ãƒ©ãƒ¼ã‚­ãƒ£ãƒƒãƒãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹")
            // 1. Firestoreã‹ã‚‰ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å…¨ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—
            let turns = try await firebaseRepository.fetchTurns(
                userId: userId,
                childId: childId,
                sessionId: sessionId
            )
            
            guard !turns.isEmpty else {
                print("âš ï¸ ConversationController: ã‚¿ãƒ¼ãƒ³ãŒå­˜åœ¨ã—ãªã„ãŸã‚åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ— - sessionId: \(sessionId)")
                return
            }
            
            print("ğŸ“Š ConversationController: å–å¾—ã—ãŸã‚¿ãƒ¼ãƒ³æ•° - \(turns.count)")
            
            // 2. ãƒ†ã‚­ã‚¹ãƒˆã‚’é€£çµã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            let conversationLog = turns.compactMap { turn -> String? in
                guard let text = turn.text, !text.isEmpty else { return nil }
                let roleLabel = turn.role == .child ? "å­ã©ã‚‚" : "AI"
                return "\(roleLabel): \(text)"
            }.joined(separator: "\n")
            
            print("ğŸ“Š ConversationController: ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã• - \(conversationLog.count)æ–‡å­—, ãƒ†ã‚­ã‚¹ãƒˆã‚ã‚Šã®ã‚¿ãƒ¼ãƒ³æ•°: \(turns.filter { $0.text != nil && !$0.text!.isEmpty }.count)")
            print("ğŸ“’ ConversationController: ä¼šè©±ãƒ­ã‚°ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå…ˆé ­150æ–‡å­—ï¼‰: \(conversationLog.prefix(150))")
            
            guard !conversationLog.isEmpty else {
                print("âš ï¸ ConversationController: ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆãŒå­˜åœ¨ã—ãªã„ãŸã‚åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ— - sessionId: \(sessionId), ã‚¿ãƒ¼ãƒ³æ•°: \(turns.count)")
                return
            }
            
            print("ğŸ“ ConversationController: ä¼šè©±ãƒ­ã‚°ï¼ˆ\(turns.count)ã‚¿ãƒ¼ãƒ³ï¼‰\n\(conversationLog)")
            
            // 3. OpenAI Chat Completion (gpt-4o-mini) ã«æŠ•ã’ã‚‹
            let prompt = """
            ä»¥ä¸‹ã®è¦ªå­ã®ä¼šè©±ãƒ­ã‚°ã‚’åˆ†æã—ã€JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
            
            å‡ºåŠ›é …ç›®:
            - summary: å­ã©ã‚‚ã®ç™ºè©±ã‚’ä¸­å¿ƒã«ã€è¦ªå‘ã‘ã«1ã€œ2è¡Œã§ç°¡æ½”ã«ã¾ã¨ã‚ã‚‹ã€‚è¿”ç­”ãŒçŸ­ã„/é›‘è«‡ãŒå°‘ãªã„å ´åˆã¯çŠ¶æ³ã ã‘çŸ­ãè§¦ã‚Œã‚‹ï¼ˆé•·ã„é£¾ã‚Šä»˜ã‘ã¯ã—ãªã„ï¼‰ã€‚
            - interests: å­ã©ã‚‚ãŒèˆˆå‘³ã‚’ç¤ºã—ãŸãƒˆãƒ”ãƒƒã‚¯ï¼ˆdinosaurs, space, cooking, animals, vehicles, music, sports, crafts, stories, insects, princess, heroes, robots, nature, others ã‹ã‚‰é¸æŠã€‚è‹±èªã®enumå€¤ã§é…åˆ—ã§å‡ºåŠ›ï¼‰
            - newWords: å­ã©ã‚‚ãŒä½¿ã£ãŸç‰¹å¾´çš„ãªå˜èªã‚„æˆé•·ã‚’æ„Ÿã˜ã‚‹è¨€è‘‰ï¼ˆ3ã¤ã¾ã§ã€é…åˆ—ã§å‡ºåŠ›ï¼‰
            
            ä¼šè©±ãƒ­ã‚°:
            \(conversationLog)
            
            JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ä¾‹:
            {
              "summary": "å­ã©ã‚‚ãŒæç«œã®ç¨®é¡ã«ã¤ã„ã¦è©³ã—ãè©±ã—ã¦ã„ã¾ã—ãŸã€‚ãƒ†ã‚£ãƒ©ãƒã‚µã‚¦ãƒ«ã‚¹ã¨ãƒˆãƒªã‚±ãƒ©ãƒˆãƒ—ã‚¹ã®é•ã„ã‚’èª¬æ˜ã—ãŸã‚Šã€è‰é£Ÿã¨è‚‰é£Ÿã®é•ã„ã«ã¤ã„ã¦èˆˆå‘³æ·±ãã†ã«è³ªå•ã—ã¦ã„ã¾ã—ãŸã€‚",
              "interests": ["dinosaurs", "animals"],
              "newWords": ["ãƒ†ã‚£ãƒ©ãƒã‚µã‚¦ãƒ«ã‚¹", "è‰é£Ÿ", "è‚‰é£Ÿ"]
            }
            """
            
            struct Payload: Encodable {
                let model: String
                let messages: [[String: String]]
                let response_format: [String: String]
                let temperature: Double
            }
            
            let payload = Payload(
                model: "gpt-4o-mini",
                messages: [
                    ["role": "system", "content": "ã‚ãªãŸã¯ä¼šè©±åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚JSONå½¢å¼ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"],
                    ["role": "user", "content": prompt]
                ],
                response_format: ["type": "json_object"],
                temperature: 0.3
            )
            
            let endpoint = (Bundle.main.object(forInfoDictionaryKey: "API_BASE") as? String)
                .flatMap(URL.init(string:)) ?? URL(string: "https://api.openai.com/v1")!
            
            var req = URLRequest(url: endpoint.appendingPathComponent("chat/completions"))
            req.httpMethod = "POST"
            req.addValue("application/json", forHTTPHeaderField: "Content-Type")
            req.addValue("Bearer \(AppConfig.openAIKey)", forHTTPHeaderField: "Authorization")
            req.httpBody = try JSONEncoder().encode(payload)
            
            let (data, response) = try await URLSession.shared.data(for: req)
            
            if let httpResponse = response as? HTTPURLResponse, httpResponse.statusCode != 200 {
                print("âŒ ConversationController: åˆ†æAPIå‘¼ã³å‡ºã—å¤±æ•— - Status: \(httpResponse.statusCode)")
                if let errorData = String(data: data, encoding: .utf8) {
                    print("   Error: \(errorData)")
                }
                return
            }
            
            // 4. JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            struct AnalysisResponse: Decodable {
                struct Choice: Decodable {
                    struct Message: Decodable {
                        let content: String
                    }
                    let message: Message
                }
                let choices: [Choice]
            }
            
            struct AnalysisResult: Decodable {
                let summary: String?
                let interests: [String]?
                let newWords: [String]?
            }
            
            let decoded = try JSONDecoder().decode(AnalysisResponse.self, from: data)
            guard let content = decoded.choices.first?.message.content else {
                print("âŒ ConversationController: åˆ†æçµæœãŒç©ºã§ã™")
                return
            }
            
            // JSONæ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹
            print("ğŸ” ConversationController: åˆ†æçµæœã®JSONæ–‡å­—åˆ— - content: \(content)")
            guard let jsonData = content.data(using: .utf8) else {
                print("âŒ ConversationController: JSONæ–‡å­—åˆ—ã®dataå¤‰æ›å¤±æ•—")
                return
            }
            
            let result: AnalysisResult
            do {
                result = try JSONDecoder().decode(AnalysisResult.self, from: jsonData)
                print("âœ… ConversationController: åˆ†æçµæœã®JSONãƒ‘ãƒ¼ã‚¹æˆåŠŸ - summary: \(result.summary ?? "nil"), interests: \(result.interests ?? []), newWords: \(result.newWords ?? [])")
            } catch {
                print("âŒ ConversationController: åˆ†æçµæœã®JSONãƒ‘ãƒ¼ã‚¹å¤±æ•— - error: \(error), content: \(content)")
                return
            }
            
            // 5. çµæœã‚’Firestoreã«ä¿å­˜
            let summaries = result.summary.map { [$0] } ?? []
            let interests = (result.interests ?? []).compactMap { FirebaseInterestTag(rawValue: $0) }
            let newVocabulary = result.newWords ?? []
            
            print("ğŸ” ConversationController: ä¿å­˜å‰ã®ãƒ‡ãƒ¼ã‚¿ - summaries: \(summaries), interests: \(interests.map { $0.rawValue }), newVocabulary: \(newVocabulary)")
            
            try await firebaseRepository.updateAnalysis(
                userId: userId,
                childId: childId,
                sessionId: sessionId,
                summaries: summaries,
                interests: interests,
                newVocabulary: newVocabulary
            )
            print("âœ… ConversationController: åˆ†æçµæœã‚’Firebaseã«ä¿å­˜å®Œäº†")
            await MainActor.run {
                if let firstSummary = summaries.first, !firstSummary.isEmpty {
                    self.liveSummary = firstSummary
                }
                self.liveInterests = interests
                self.liveNewVocabulary = newVocabulary
                print("ğŸŸ¢ ConversationController: live fields updated - summary:'\(self.liveSummary)', interests:\(self.liveInterests.map { $0.rawValue }), newVocabulary:\(self.liveNewVocabulary)")
            }
            print("âœ… ConversationController: ä¼šè©±åˆ†æå®Œäº† - summary: \(summaries.first ?? "ãªã—"), interests: \(interests.map { $0.rawValue }), vocabulary: \(newVocabulary)")
            
        } catch {
            print("âŒ ConversationController: analyzeSession - ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: \(error)")
            print("âŒ ConversationController: analyzeSession - ã‚¨ãƒ©ãƒ¼ã®è©³ç´°: \(String(describing: error))")
            if let nsError = error as NSError? {
                print("âŒ ConversationController: analyzeSession - NSErrorè©³ç´° - domain: \(nsError.domain), code: \(nsError.code), userInfo: \(nsError.userInfo)")
            }
            logFirebaseError(error, operation: "ä¼šè©±åˆ†æ")
        }
    }
}

// MARK: - gpt-4o-audio-preview ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
fileprivate final class AudioPreviewStreamingClient {
    private let apiKey: String
    private let apiBase: URL
    private let decoder = JSONDecoder()
    // ğŸ”Š éŸ³å£°ç”Ÿæˆã‚’å¼·ãä¿ƒã™éš ã—ãƒªãƒã‚¤ãƒ³ãƒ‰ï¼ˆæ¯ã‚¿ãƒ¼ãƒ³å…ˆé ­ã«æŒ¿å…¥ï¼‰
    private let audioReminder = "å¿…ãšéŸ³å£°ã¤ãã§è¿”ã—ã¦ã€‚éŸ³å£°ãŒä½œã‚Œãªã„ãªã‚‰ã€å†…å®¹ã‚’çŸ­ãã—ã¦ã§ã‚‚éŸ³å£°ã‚’å‡ºã—ã¦ã€‚"
    
    struct AudioPreviewResult {
        let text: String
        let audioMissing: Bool
    }
    
    init(apiKey: String, apiBase: URL) {
        self.apiKey = apiKey
        self.apiBase = apiBase
    }
    
    func streamResponse(
        audioData: Data,
        systemPrompt: String,
        history: [ConversationController.HistoryItem],
        onText: @escaping (String) -> Void,
        onAudioChunk: @escaping (Data) -> Void,
        onFirstByte: ((Date) -> Void)? = nil,
        emitText: Bool = true
    ) async throws -> AudioPreviewResult {
        var messages: [AudioPreviewPayload.Message] = []
        messages.append(.init(role: "system", content: [.text(systemPrompt)]))
        for item in history {
            let role = (item.role == "assistant") ? "assistant" : "user"
            messages.append(.init(role: role, content: [.text(item.text)]))
        }
        // âœ… éŸ³å£°å‡ºåŠ›ã‚’é€ƒã•ãªã„ãŸã‚ã®ãƒªãƒã‚¤ãƒ³ãƒ‰ã‚’è¿½åŠ 
        messages.append(.init(role: "user", content: [.text(audioReminder)]))
        messages.append(.init(role: "user", content: [.inputAudio(.init(data: audioData.base64EncodedString(), format: "wav"))]))

        return try await stream(
            messages: messages,
            inputSummary: "audioBytes=\(audioData.count)",
            emitText: emitText,
            onText: onText,
            onAudioChunk: onAudioChunk,
            onFirstByte: onFirstByte
        )
    }
    
    func streamResponseText(
        userText: String,
        systemPrompt: String,
        history: [ConversationController.HistoryItem],
        onText: @escaping (String) -> Void,
        onAudioChunk: @escaping (Data) -> Void,
        onFirstByte: ((Date) -> Void)? = nil,
        emitText: Bool = true
    ) async throws -> AudioPreviewResult {
        var messages: [AudioPreviewPayload.Message] = []
        messages.append(.init(role: "system", content: [.text(systemPrompt)]))
        for item in history {
            let role = (item.role == "assistant") ? "assistant" : "user"
            messages.append(.init(role: role, content: [.text(item.text)]))
        }
        // âœ… éŸ³å£°å‡ºåŠ›ã‚’é€ƒã•ãªã„ãŸã‚ã®ãƒªãƒã‚¤ãƒ³ãƒ‰ã‚’è¿½åŠ 
        messages.append(.init(role: "user", content: [.text(audioReminder)]))
        messages.append(.init(role: "user", content: [.text(userText)]))
        
        return try await stream(
            messages: messages,
            inputSummary: "textChars=\(userText.count)",
            emitText: emitText,
            onText: onText,
            onAudioChunk: onAudioChunk,
            onFirstByte: onFirstByte
        )
    }
    
    private func stream(
        messages: [AudioPreviewPayload.Message],
        inputSummary: String,
        emitText: Bool,
        onText: @escaping (String) -> Void,
        onAudioChunk: @escaping (Data) -> Void,
        onFirstByte: ((Date) -> Void)? = nil
    ) async throws -> AudioPreviewResult {
        var request = URLRequest(url: completionsURL())
        request.httpMethod = "POST"
        request.addValue("application/json", forHTTPHeaderField: "Content-Type")
        request.addValue("text/event-stream", forHTTPHeaderField: "Accept")
        request.addValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        // audio-previewå°‚ç”¨ãƒ˜ãƒƒãƒ€ï¼ˆç’°å¢ƒã«ã‚ˆã£ã¦ä¸è¦ãªå ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆå¯ï¼‰
        request.addValue("audio-preview", forHTTPHeaderField: "OpenAI-Beta")
        // éŸ³å£°ç”ŸæˆãŒãƒ†ã‚­ã‚¹ãƒˆã‚ˆã‚Šé…ã‚ŒãŸå ´åˆã®ãŸã‚ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å°‘ã—é•·ã‚ã«
        request.timeoutInterval = 60.0
        
        let t0 = Date()
        
        let payload = AudioPreviewPayload(
            model: "gpt-4o-audio-preview",
            stream: true,
            modalities: ["text", "audio"],
            // å‡ºåŠ›ã¯ãƒ˜ãƒƒãƒ€ãªã—PCM16ã§å—ä¿¡ã™ã‚‹ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å†ç”ŸãŒå®‰å®šï¼‰
            audio: .init(voice: "nova", format: "pcm16"),
            messages: messages
        )
        request.httpBody = try JSONEncoder().encode(payload)
        print("â±ï¸ AudioPreviewStreamingClient: stream start - \(inputSummary)")
        print("ğŸ¯ AudioPreviewStreamingClient: request config - model=\(payload.model), modalities=\(payload.modalities), audio.voice=\(payload.audio.voice), audio.format=\(payload.audio.format), messages=\(messages.count)")
        
        var finalTextClean = ""
        var didReceiveAudio = false
        var textChunkCount = 0
        var audioChunkCount = 0
        var emptyChunkCount = 0
        var audioFieldNullCount = 0
        var finishReasons: [String: Int] = [:]
        var refusalSummaries: [String] = []
        var audioMissingPayloadSamples: [String] = []
        let (bytes, response) = try await URLSession.shared.bytes(for: request)
        let tReqDone = Date()
        print("â±ï¸ AudioPreviewStreamingClient: request sent -> awaiting first byte (\(String(format: "%.2f", tReqDone.timeIntervalSince(t0)))s)")
        guard let http = response as? HTTPURLResponse else {
            throw NSError(domain: "AudioPreviewStreamingClient", code: -1, userInfo: [NSLocalizedDescriptionKey: "ä¸æ­£ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§ã™"])
        }
        onFirstByte?(tReqDone)
        print("ğŸ“¦ AudioPreviewStreamingClient: response status=\(http.statusCode), headers=\(http.allHeaderFields)")
        if !(200..<300).contains(http.statusCode) {
            // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒœãƒ‡ã‚£ã‚’æ–‡å­—åˆ—åŒ–
            let bodyString: String
            if let data = try? await bytes.reduce(into: Data(), { $0.append($1) }),
               let str = String(data: data, encoding: .utf8) {
                bodyString = str
            } else {
                bodyString = "(bodyãªã—)"
            }
            print("âŒ AudioPreviewStreamingClient: HTTP \(http.statusCode) - body: \(bodyString)")
            throw NSError(domain: "AudioPreviewStreamingClient", code: http.statusCode, userInfo: [NSLocalizedDescriptionKey: "HTTP \(http.statusCode)"])
        }
        
        for try await line in bytes.lines {
            guard line.hasPrefix("data:") else { continue }
            let payloadString = line.dropFirst(5).trimmingCharacters(in: .whitespaces)
            if payloadString == "[DONE]" { break }
            guard let data = payloadString.data(using: .utf8) else {
                print("âš ï¸ AudioPreviewStreamingClient: payloadString decodeå¤±æ•— len=\(payloadString.count)")
                continue
            }
            
            // choicesã®ãªã„ã‚¤ãƒ™ãƒ³ãƒˆã‚’å…ˆã«ãµã‚‹ã„åˆ†ã‘ï¼ˆã‚¨ãƒ©ãƒ¼/ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆç­‰ï¼‰
            if let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any] {
                if json["choices"] == nil {
                    let type = json["type"] ?? json["event"] ?? json["object"] ?? "(unknown)"
                    if json["error"] != nil {
                        print("âš ï¸ AudioPreviewStreamingClient: non-choice event (error) type=\(type)")
                    } else {
                        print("â„¹ï¸ AudioPreviewStreamingClient: non-choice event skipped type=\(type)")
                    }
                    continue
                }
            }
            
            do {
                let chunk = try decoder.decode(AudioPreviewStreamChunk.self, from: data)
                for choice in chunk.choices {
                    if let fr = choice.finishReason {
                        finishReasons[fr, default: 0] += 1
                    }
                    if let refusal = choice.message?.refusal {
                        let summary = "reason=\(refusal.reason ?? "nil"), message=\(refusal.message ?? "nil")"
                        refusalSummaries.append(summary)
                    }
                    
                    if let messageAudio = choice.message?.audio {
                        if let audioString = messageAudio.data {
                            if let audioData = Data(base64Encoded: audioString) {
                                didReceiveAudio = true
                                audioChunkCount += 1
                                onAudioChunk(audioData)
                            } else {
                                print("âš ï¸ AudioPreviewStreamingClient: message.audio decodeå¤±æ•— - length=\(audioString.count)")
                            }
                        } else {
                            audioFieldNullCount += 1
                            print("âš ï¸ AudioPreviewStreamingClient: message.audio present but data=null")
                        }
                    }
                    
                    guard let delta = choice.delta else { continue }
                    var textFragments: [String] = []
                    
                    if let parts = delta.content {
                        textFragments.append(contentsOf: parts.compactMap { $0.text })
                    }
                    if let contentString = delta.contentString, !contentString.isEmpty {
                        textFragments.append(contentString)
                    }
                    if let outputs = delta.outputText {
                        for block in outputs {
                            if let blockText = block.text, !blockText.isEmpty {
                                textFragments.append(blockText)
                            }
                            if let parts = block.content {
                                textFragments.append(contentsOf: parts.compactMap { $0.text })
                            }
                        }
                    }
                    if let transcript = delta.audio?.transcript, !transcript.isEmpty {
                        textFragments.append(transcript)
                    }
                    
                    let mergedRaw = textFragments.joined()
                    let merged = sanitizeJapanese(mergedRaw)
                    let ratio = mergedRaw.isEmpty ? 0.0 : Double(merged.count) / Double(mergedRaw.count)
                    if merged.isEmpty {
                        // drop pure noise
                    } else if ratio < 0.5 {
                        print("âš ï¸ AudioPreviewStreamingClient: text delta dropped (non-ja dominant). raw='\(mergedRaw.prefix(60))...'")
                    } else {
                        print("ğŸ“ AudioPreviewStreamingClient: text delta (ja) = \(merged)")
                        finalTextClean += merged
                        textChunkCount += 1
                        if emitText { onText(merged) }
                    }
                    
                    if merged.isEmpty {
                        print("âš ï¸ AudioPreviewStreamingClient: no text in chunk. contentParts=\(delta.content?.count ?? 0), contentString=\(delta.contentString ?? "nil"), outputText=\(delta.outputText?.count ?? 0)")
                        print("   raw payload omitted (base64 audio may be large)")
                    }
                    
                    if let audioString = delta.audio?.data {
                        if let audioData = Data(base64Encoded: audioString) {
                            didReceiveAudio = true
                            audioChunkCount += 1
                            onAudioChunk(audioData)
                        } else {
                            print("âš ï¸ AudioPreviewStreamingClient: audio chunk decodeå¤±æ•— - length=\(audioString.count)")
                        }
                    } else if delta.audio != nil {
                        audioFieldNullCount += 1
                        print("âš ï¸ AudioPreviewStreamingClient: delta.audio present but data=null")
                    }
                    if delta.audio?.data == nil && (delta.content?.isEmpty ?? true) && (delta.outputText?.isEmpty ?? true) {
                        print("âš ï¸ AudioPreviewStreamingClient: chunk has no text/audio; skipping")
                        emptyChunkCount += 1
                    }
                }
            } catch {
                print("âš ï¸ AudioPreviewStreamingClient: ãƒãƒ£ãƒ³ã‚¯ãƒ‘ãƒ¼ã‚¹å¤±æ•— - \(error)")
                if payloadString.count < 200 { print("   payloadString='\(payloadString)'") }
            }
            
            if !didReceiveAudio && audioMissingPayloadSamples.count < 3 {
                audioMissingPayloadSamples.append(Self.redactedPayloadSample(payloadString))
            }
        }
        
        if !didReceiveAudio {
            print("âŒ AudioPreviewStreamingClient: éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ãªã—ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®å¿œç­”ï¼‰ - model=\(payload.model), modalities=\(payload.modalities), audio.voice=\(payload.audio.voice), audio.format=\(payload.audio.format)")
        }
        print("ğŸ“Š AudioPreviewStreamingClient: chunk summary -> text:\(textChunkCount), audio:\(audioChunkCount), empty:\(emptyChunkCount), audioMissing=\(!didReceiveAudio)")
        if audioFieldNullCount > 0 {
            print("ğŸ“„ AudioPreviewStreamingClient: audio objects without data count=\(audioFieldNullCount)")
        }
        if !finishReasons.isEmpty {
            let summary = finishReasons.map { "\($0.key):\($0.value)" }.joined(separator: ", ")
            print("ğŸ“„ AudioPreviewStreamingClient: finish_reason summary -> \(summary)")
        }
        if !refusalSummaries.isEmpty {
            print("ğŸ“„ AudioPreviewStreamingClient: refusal summaries (\(refusalSummaries.count)) -> \(refusalSummaries.prefix(3).joined(separator: " | "))")
        }
        if !audioMissingPayloadSamples.isEmpty && !didReceiveAudio {
            print("ğŸ§ª AudioPreviewStreamingClient: audioMissing payload samples (redacted) ->")
            audioMissingPayloadSamples.forEach { print("   \( $0 )") }
        }
        if let contentType = http.value(forHTTPHeaderField: "Content-Type") {
            print("ğŸ“¦ AudioPreviewStreamingClient: response headers - Content-Type: \(contentType)")
        }
        
        let final = finalTextClean.isEmpty ? "(ãŠã¸ã‚“ã˜ãŒã§ããªã‹ã£ãŸã‚ˆ)" : finalTextClean
        return AudioPreviewResult(text: final, audioMissing: !didReceiveAudio)
    }
    
    private func completionsURL() -> URL {
        if apiBase.path.contains("/v1") {
            return apiBase.appendingPathComponent("chat/completions")
        } else {
            return apiBase
                .appendingPathComponent("v1")
                .appendingPathComponent("chat/completions")
        }
    }
    
    private static func redactedPayloadSample(_ payload: String) -> String {
        var s = payload
        while let range = s.range(of: "\"data\":\"") {
            if let endQuote = s[range.upperBound...].firstIndex(of: "\"") {
                let replacement = "\"data\":\"<base64:\(s[range.upperBound..<endQuote].count) bytes>\""
                s.replaceSubrange(range.lowerBound...endQuote, with: replacement)
            } else {
                break
            }
        }
        if s.count > 240 {
            let prefix = s.prefix(240)
            return "\(prefix)...(truncated, len=\(s.count))"
        }
        return s
    }
    
    private static func hexSnippet(_ data: Data, length: Int) -> String {
        guard !data.isEmpty else { return "(empty)" }
        return data.prefix(length).map { String(format: "%02X", $0) }.joined(separator: " ")
    }
    
    /// TTSå¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«24kHz/mono/PCM16ãƒ‡ãƒ¼ã‚¿ã¸å¤‰æ›
    private static func pcm16Data(from data: Data, contentType: String?) -> Data? {
        // JSONã‚„æ˜ã‚‰ã‹ãªã‚¨ãƒ©ãƒ¼ã‚’å¼¾ã
        if let first = data.first, first == UInt8(ascii: "{") || first == UInt8(ascii: "[") {
            return nil
        }
        
        // WAVãªã‚‰AVAudioFileã§æ­£è¦åŒ–
        let isWav = (contentType?.lowercased().contains("wav") == true) ||
            (data.count >= 4 && String(data: data.prefix(4), encoding: .ascii) == "RIFF")
        if isWav {
            let tmpURL = FileManager.default.temporaryDirectory.appendingPathComponent("tts_fallback_\(UUID().uuidString).wav")
            do {
                try data.write(to: tmpURL)
                let file = try AVAudioFile(forReading: tmpURL)
                let targetFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, sampleRate: 24_000, channels: 1, interleaved: true)!
                let converter = AVAudioConverter(from: file.processingFormat, to: targetFormat)!
                
                var pcmData = Data()
                while true {
                    guard let inputBuffer = AVAudioPCMBuffer(pcmFormat: file.processingFormat, frameCapacity: 1024) else { break }
                    try file.read(into: inputBuffer)
                    if inputBuffer.frameLength == 0 { break }
                    
                    let ratio = targetFormat.sampleRate / file.processingFormat.sampleRate
                    let outFrames = AVAudioFrameCount(Double(inputBuffer.frameLength) * ratio + 16)
                    guard let outBuffer = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: outFrames) else { break }
                    
                    var error: NSError?
                    let status = converter.convert(to: outBuffer, error: &error) { _, outStatus in
                        outStatus.pointee = .haveData
                        return inputBuffer
                    }
                    if status == .haveData || status == .endOfStream,
                       let ch = outBuffer.int16ChannelData?.pointee {
                        let sampleCount = Int(outBuffer.frameLength) * Int(targetFormat.channelCount)
                        pcmData.append(UnsafeBufferPointer(start: ch, count: sampleCount))
                    } else if let error {
                        print("âš ï¸ ConversationController: WAV->PCM convert error - \(error.localizedDescription)")
                        return nil
                    }
                }
                try? FileManager.default.removeItem(at: tmpURL)
                return pcmData.isEmpty ? nil : pcmData
            } catch {
                print("âš ï¸ ConversationController: WAV decode failed - \(error.localizedDescription)")
                try? FileManager.default.removeItem(at: tmpURL)
                return nil
            }
        }
        
        // PCM16å‰æã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆï¼ˆaudio/pcm ç­‰ï¼‰
        let isPCM = contentType?.lowercased().contains("pcm") == true || contentType?.lowercased().contains("audio/raw") == true
        if isPCM {
            return data
        }
        
        // ä¸æ˜å½¢å¼ã¯ç„¡éŸ³æ‰±ã„
        print("âš ï¸ ConversationController: unknown TTS format (contentType=\(contentType ?? "nil")), cannot decode safely")
        return nil
    }
    
    /// ç°¡æ˜“ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼ˆæ—¥æœ¬èªä¸­å¿ƒã®æ–‡å­—ã ã‘æ®‹ã™ï¼‰
    private func sanitizeJapanese(_ text: String) -> String {
        if text.isEmpty { return text }
        var allowed = CharacterSet()
        allowed.formUnion(.whitespacesAndNewlines)
        allowed.formUnion(CharacterSet(charactersIn: "ã€‚ã€ï¼ï¼Ÿãƒ»ãƒ¼ã€Œã€ã€ã€ï¼ˆï¼‰ï¼»ï¼½ã€ã€‘â€¦ã€œ"))
        // ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠ
        allowed.formUnion(CharacterSet(charactersIn: "\u{3040}"..."\u{30FF}"))
        // åŠè§’ã‚«ã‚¿ã‚«ãƒŠ
        allowed.formUnion(CharacterSet(charactersIn: "\u{FF65}"..."\u{FF9F}"))
        // CJKçµ±åˆæ¼¢å­—
        allowed.formUnion(CharacterSet(charactersIn: "\u{4E00}"..."\u{9FFF}"))
        
        let cleanedScalars = text.unicodeScalars.filter { allowed.contains($0) }
        return String(String.UnicodeScalarView(cleanedScalars))
    }
}

// MARK: - Payload/Stream Models
private struct AudioPreviewPayload: Encodable {
    struct AudioConfig: Encodable {
        let voice: String
        let format: String
    }
    
    struct Message: Encodable {
        let role: String
        let content: [MessageContent]
    }
    
    enum MessageContent: Encodable {
        case text(String)
        case inputAudio(AudioData)
        
        struct AudioData: Encodable {
            let data: String
            let format: String
        }
        
        func encode(to encoder: Encoder) throws {
            var container = encoder.container(keyedBy: CodingKeys.self)
            switch self {
            case .text(let text):
                try container.encode("text", forKey: .type)
                try container.encode(text, forKey: .text)
            case .inputAudio(let audio):
                try container.encode("input_audio", forKey: .type)
                // OpenAI audio-preview expects input_audio payload under key "input_audio"
                try container.encode(audio, forKey: .inputAudio)
            }
        }
        
        enum CodingKeys: String, CodingKey { case type, text, audio, inputAudio = "input_audio" }
    }
    
    let model: String
    let stream: Bool
    let modalities: [String]
    let audio: AudioConfig
    let messages: [Message]
}

private struct AudioPreviewStreamChunk: Decodable {
    struct Choice: Decodable {
        struct Delta: Decodable {
            let content: [ContentPart]?
            let contentString: String?
            let outputText: [OutputText]?
            let audio: AudioDelta?
            
            enum CodingKeys: String, CodingKey {
                case content
                case outputText = "output_text"
                case audio
            }
            
            init(from decoder: Decoder) throws {
                let container = try decoder.container(keyedBy: CodingKeys.self)
                if let parts = try? container.decode([ContentPart].self, forKey: .content) {
                    self.content = parts
                    self.contentString = nil
                } else {
                    self.content = nil
                    self.contentString = try? container.decode(String.self, forKey: .content)
                }
                self.outputText = try? container.decode([OutputText].self, forKey: .outputText)
                self.audio = try? container.decode(AudioDelta.self, forKey: .audio)
            }
        }
        let delta: Delta?
        let message: Message?
        let finishReason: String?
        
        enum CodingKeys: String, CodingKey {
            case delta
            case message
            case finishReason = "finish_reason"
        }
    }
    
    struct ContentPart: Decodable {
        let type: String?
        let text: String?
    }
    
    struct AudioDelta: Decodable {
        let id: String?
        let data: String?
        let transcript: String?
        
        enum CodingKeys: String, CodingKey {
            case id
            case data
            case transcript
        }
    }
    
    struct OutputText: Decodable {
        let id: String?
        let content: [OutputTextPart]?
        let text: String?
    }
    
    struct OutputTextPart: Decodable {
        let type: String?
        let text: String?
    }
    
    struct Message: Decodable {
        let audio: AudioDelta?
        let refusal: Refusal?
        let content: [ContentPart]?
    }
    
    struct Refusal: Decodable {
        let reason: String?
        let message: String?
    }
    
    let choices: [Choice]
}
