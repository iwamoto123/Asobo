import AVFoundation


/// âœ… ãƒã‚¤ã‚¯å…¥åŠ›ã‚­ãƒ£ãƒ—ãƒãƒ£ã¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›
/// 
/// ## é‡è¦ãªè¨­å®šãƒã‚¤ãƒ³ãƒˆ
/// 1. **å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: 24kHz/mono/PCM16LEï¼ˆOpenAI Realtime APIã®è¦æ±‚ä»•æ§˜ï¼‰
/// 2. **ãƒãƒƒãƒé€ä¿¡**: 60msã”ã¨ã«ã¾ã¨ã‚ã¦é€ä¿¡ï¼ˆåå¿œé€Ÿåº¦é‡è¦–ã€AECãŒåŠ¹ã„ã¦ã„ã‚‹ãŸã‚çŸ­ç¸®å¯èƒ½ï¼‰
/// 3. **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›**: AVAudioConverterã§ç¢ºå®Ÿã«24kHz/mono/PCM16ã«å¤‰æ›
/// 
/// ## å¤‰æ›ãƒ•ãƒ­ãƒ¼
/// - å…¥åŠ›: ãƒ‡ãƒã‚¤ã‚¹ä¾å­˜ï¼ˆé€šå¸¸48kHz/mono/Float32ã€AECé©ç”¨å¾Œï¼‰
/// - ã‚¨ãƒ³ã‚¸ãƒ³å†…: 48kHz/monoã§å‡¦ç†ï¼ˆAECæœ€é©åŒ–ã®ãŸã‚ï¼‰
/// - é€ä¿¡æ™‚å¤‰æ›: AVAudioConverterã§24kHz/mono/PCM16LEã«å¤‰æ›ï¼ˆã‚µãƒ¼ãƒé€ä¿¡ã®ç›´å‰ï¼‰
/// - ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°: 60msåˆ†ã‚’ã¾ã¨ã‚ã¦é€ä¿¡ï¼ˆåå¿œé€Ÿåº¦é‡è¦–ï¼‰
/// - å‡ºåŠ›: 24kHz/mono/PCM16LEå½¢å¼ã®AVAudioPCMBuffer
/// 
/// ## AECå¯¾ç­–
/// - **ã‚¨ãƒ³ã‚¸ãƒ³å†…ã¯48kHz/monoã§çµ±ä¸€**: AECã¯48kHz/ãƒ¢ãƒã®ãƒ‘ã‚¹ã§æœ€ã‚‚å®‰å®š
/// - **å†ç”Ÿä¸­ã®ãƒã‚¤ã‚¯é€ä¿¡ã‚²ãƒ¼ãƒˆ**: AIå†ç”Ÿä¸­ã¯ãƒã‚¤ã‚¯å…¥åŠ›ã‚’ã‚µãƒ¼ãƒã«é€ä¿¡ã—ãªã„ï¼ˆãƒãƒ¼ãƒ•ãƒ‡ãƒ¥ãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ï¼‰
/// - **ãƒãƒ¼ã‚¸ã‚¤ãƒ³æ¤œå‡º**: ãƒãƒ¼ã‚¸ã‚¤ãƒ³æ¤œå‡ºæ™‚ã®ã¿é€ä¿¡ã‚’å†é–‹
public final class MicrophoneCapture {
  private let engine: AVAudioEngine
  private var ownsEngine: Bool  // ã‚¨ãƒ³ã‚¸ãƒ³ã®æ‰€æœ‰æ¨©ã‚’æŒã¤ã‹ã©ã†ã‹
  private let outFormat: AVAudioFormat
  private var converter: AVAudioConverter?  // âœ… ã‚¨ãƒ³ã‚¸ãƒ³é–‹å§‹å¾Œã«å†ä½œæˆã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€varã«å¤‰æ›´
  private let onPCM: (AVAudioPCMBuffer) -> Void
  // âœ… è¿½åŠ : éŸ³é‡ãƒ¬ãƒ™ãƒ«ï¼ˆdBï¼‰ã‚’é€šçŸ¥ã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  public var onVolume: ((Double) -> Void)?
  // âœ… è¿½åŠ : VADç¢ºç‡ã‚’é€šçŸ¥ã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  public var onVADProbability: ((Float) -> Void)?  // Silero VAD ã®ç”Ÿç¢ºç‡ã‚’é€šçŸ¥ï¼ˆUI/Viz ç”¨ï¼‰
  // âœ… è¿½åŠ : ãƒãƒ¼ã‚¸ã‚¤ãƒ³æ¤œçŸ¥æ™‚ã«å‘¼ã³å‡ºã™ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  public var onBargeIn: (() -> Void)?
  private var running = false
  private var vad: SileroVAD?
  private var vadBuffer16k: [Float] = []
  private let vadChunkSize = 512
  private let vadTargetSampleRate: Double = 16_000
  private let vadQueue = DispatchQueue(label: "com.asobo.audio.vad")
  private let vadQueueSpecificKey = DispatchSpecificKey<Bool>()
  private var lastVADSignalLogTime: Date?
  private var vadLogCounter: Int = 0
  private var vadConverter: AVAudioConverter?
  // âœ… VADç¢ºç‡ã®æœ€æ–°å€¤ï¼ˆãƒãƒ¼ã‚¸ã‚¤ãƒ³åˆ¤å®šã«ã‚‚å…±é€šåˆ©ç”¨ï¼‰
  private var latestVADProbability: Float = 0.0
  private let vadProbabilityLock = NSLock()
  // âœ… Silero VAD ã«ã‚ˆã‚‹å‰²ã‚Šè¾¼ã¿åˆ¤å®šç”¨ï¼ˆé€£ç¶šæ¤œå‡ºãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹ï¼‰
  private let bargeInVADThreshold: Float = 0.5
  private let bargeInHoldDuration: TimeInterval = 0.15  // 150ms ä»¥ä¸Šé€£ç¶šã§speechã‚’æ¤œå‡ºã—ãŸã‚‰ãƒãƒ¼ã‚¸ã‚¤ãƒ³
  private var vadSpeechStartTimeForBargeIn: Date?
  
  // âœ… ãƒãƒƒãƒé€ä¿¡ç”¨ï¼š60msã”ã¨ã«ã¾ã¨ã‚ã¦é€ä¿¡ï¼ˆåå¿œé€Ÿåº¦é‡è¦–ï¼‰
  // å¤‰æ›´å‰: 200ms (å®‰å®šé‡è¦–)
  // å¤‰æ›´å¾Œ: 60ms (åå¿œé€Ÿåº¦é‡è¦–ã€AECãŒåŠ¹ã„ã¦ã„ã‚‹ãŸã‚çŸ­ç¸®å¯èƒ½)
  private var audioBuffer: Data = Data()
  private let batchDurationMs: Double = 60.0  // 60msãƒãƒƒãƒ
  private var lastFlushTime: Date = Date()
  private let flushQueue = DispatchQueue(label: "com.asobo.audio.flush")
  
  // âœ… AECå¯¾ç­–ï¼šå†ç”Ÿä¸­ã‚²ãƒ¼ãƒˆåˆ¶å¾¡
  private var isAIPlayingAudio: Bool = false
  private var userBargeIn: Bool = false
  private var outputMonitor: OutputMonitor?
  // âœ… ãƒãƒ¼ã‚¸ã‚¤ãƒ³åˆ¤å®šã®è£œåŠ©ãƒ­ã‚°ç”¨ã«å…¥åŠ›RMSã®çŸ­æœŸå±¥æ­´ã‚’ä¿æŒ
  private var recentInputRMS: [Double] = []  // ç›´è¿‘60msã®å…¥åŠ›RMS
  // å¤‰æ›´å‰: 10 (ç´„200msã®å¹³å‡ã‚’è¦‹ã‚‹ãŸã‚é…ã„)
  // å¤‰æ›´å¾Œ: 3 (ç´„60msã®å¹³å‡ã§åˆ¤æ–­ã€ä¸€ç¬ã®ç™ºè©±ã«åå¿œã•ã›ã‚‹)
  private let rmsWindowSize: Int = 3  // 3ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆç´„60msï¼‰
  private var isFirstBuffer: Bool = true  // âœ… åˆå›ãƒãƒƒãƒ•ã‚¡å—ä¿¡ãƒ•ãƒ©ã‚°ï¼ˆconverterä½œæˆã®ãŸã‚ï¼‰
  // âœ… å†ç”Ÿä¸­ã¯VoiceProcessingï¼ˆã‚³ãƒ³ãƒ•ã‚©ãƒ¼ãƒˆãƒã‚¤ã‚ºå«ã‚€ï¼‰ã‚’ã‚ªãƒ•ã«ã—ã¦ãƒã‚¤ã‚ºæºã‚’æ¸›ã‚‰ã™
  //    å†ç”Ÿä¸­ã¯é€ä¿¡ã‚²ãƒ¼ãƒˆã§ãƒã‚¤ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ã‚µãƒ¼ãƒã«å‡ºã•ãªã„ãŸã‚ã€AECã‚’ä¸€æ™‚åœæ­¢ã—ã¦ã‚‚ã‚¨ã‚³ãƒ¼ã®ãƒªã‚¹ã‚¯ã¯ä½ã„æƒ³å®š
  // âš ï¸ playbackä¸­ã®åˆ‡æ›¿ã§-10849ãŒé »ç™ºã—AVAudioEngineãŒä¸å®‰å®šã«ãªã‚‹ãŸã‚ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
  private let disableVoiceProcessingDuringPlayback: Bool = false
  
  // âœ… åˆå›æ¥ç¶šæ™‚ã®éŸ³å£°èªè­˜å•é¡Œå¯¾ç­–ï¼šãƒã‚¤ã‚¯é–‹å§‹ç›´å¾Œã®åˆæœŸãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚¹ã‚­ãƒƒãƒ—
  private var startTime: Date?  // ãƒã‚¤ã‚¯é–‹å§‹æ™‚åˆ»
  private let initialSkipDurationMs: Double = 200.0  // é–‹å§‹å¾Œ200msã¯éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡ã—ãªã„ï¼ˆåˆæœŸãƒã‚¤ã‚ºå¯¾ç­–ï¼‰

  /// âœ… å…±é€šã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼ˆAECæœ‰åŠ¹åŒ–ã®ãŸã‚æ¨å¥¨ï¼‰
  public init?(sharedEngine: AVAudioEngine, onPCM: @escaping (AVAudioPCMBuffer) -> Void, outputMonitor: OutputMonitor? = nil, ownsEngine: Bool = false) {
    self.engine = sharedEngine
    self.ownsEngine = ownsEngine
    self.onPCM = onPCM
    self.outputMonitor = outputMonitor
    // âœ… å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯å›ºå®šï¼ˆ24kHz/mono/PCM16ï¼‰
    guard let out = AVAudioFormat(commonFormat: .pcmFormatInt16,
                                  sampleRate: 24_000,  // âœ… 24kHzã«å¤‰æ›´ï¼ˆOpenAI Realtime APIã®è¦æ±‚ä»•æ§˜ã«åˆã‚ã›ã‚‹ï¼‰
                                  channels: 1,
                                  interleaved: true) else {
      return nil
    }
    self.outFormat = out
    // âœ… converterã¯start()ã§ä½œæˆï¼ˆã‚¨ãƒ³ã‚¸ãƒ³é–‹å§‹å¾Œã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä½¿ç”¨ï¼‰
    self.converter = nil
    self.vad = try? SileroVAD()
    // âœ… VADãƒãƒƒãƒ•ã‚¡ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’vadQueueã«ç›´åˆ—åŒ–ã™ã‚‹ãŸã‚ã®ãƒãƒ¼ã‚«ãƒ¼
    vadQueue.setSpecific(key: vadQueueSpecificKey, value: true)
  }
  
  /// âœ… ç‹¬è‡ªã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
  public convenience init?(onPCM: @escaping (AVAudioPCMBuffer) -> Void, outputMonitor: OutputMonitor? = nil) {
    let engine = AVAudioEngine()
    self.init(sharedEngine: engine, onPCM: onPCM, outputMonitor: outputMonitor, ownsEngine: true)
  }
  
  /// âœ… AIå†ç”ŸçŠ¶æ…‹ã‚’è¨­å®š
  public func setAIPlayingAudio(_ isPlaying: Bool) {
    isAIPlayingAudio = isPlaying
    if !isPlaying {
      // å†ç”Ÿçµ‚äº†æ™‚ã«ãƒãƒ¼ã‚¸ã‚¤ãƒ³ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
      userBargeIn = false
      recentInputRMS.removeAll()
      vadSpeechStartTimeForBargeIn = nil
    }
    
    // âœ… å†ç”Ÿä¸­ã¯VoiceProcessingï¼ˆã‚³ãƒ³ãƒ•ã‚©ãƒ¼ãƒˆãƒã‚¤ã‚ºç”Ÿæˆæºï¼‰ã‚’ä¸€æ™‚åœæ­¢
    if disableVoiceProcessingDuringPlayback {
      setVoiceProcessingEnabled(!isPlaying)
    }
  }
  
  /// âœ… RMSè¨ˆç®—ï¼ˆdBFSï¼‰
  private func calculateRMS(from buffer: AVAudioPCMBuffer) -> Double {
    guard let channelData = buffer.floatChannelData else { return -60.0 }
    let channelCount = Int(buffer.format.channelCount)
    let frameLength = Int(buffer.frameLength)
    
    var sum: Float = 0.0
    for ch in 0..<channelCount {
      let channel = channelData[ch]
      for i in 0..<frameLength {
        let sample = channel[i]
        sum += sample * sample
      }
    }
    
    let mean = sum / Float(frameLength * channelCount)
    let rms = sqrt(mean)
    
    // dBFSã«å¤‰æ›
    if rms < 1e-10 {
      return -60.0
    }
    return 20.0 * log10(Double(rms))
  }
  
  /// âœ… ãƒãƒ¼ã‚¸ã‚¤ãƒ³åˆ¤å®š
  private func checkBargeIn(inputRMS: Double, outputRMS _: Double, vadProbability: Float) -> Bool {
    // VADãŒä¸€å®šä»¥ä¸Šã§ã€Œäººå£°ã€ã¨åˆ¤å®šã•ã‚ŒãŸå ´åˆã®ã¿ã€ä¸€å®šæ™‚é–“ç¶™ç¶šã‚’å¾…ã£ã¦ã‹ã‚‰è¨±å¯
    let now = Date()
    guard vadProbability >= bargeInVADThreshold else {
      vadSpeechStartTimeForBargeIn = nil
      return false
    }

    // ç›´è¿‘60msã®å…¥åŠ›RMSã‚’è¨˜éŒ²ï¼ˆåå¿œé€Ÿåº¦é‡è¦–ï¼‰
    recentInputRMS.append(inputRMS)
    if recentInputRMS.count > rmsWindowSize {
      recentInputRMS.removeFirst()
    }
    
    // VADãŒç¶™ç¶šã—ã¦ã„ã‚‹æ™‚é–“ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹ï¼‰
    if vadSpeechStartTimeForBargeIn == nil {
      vadSpeechStartTimeForBargeIn = now
      return false
    }
    let elapsed = now.timeIntervalSince(vadSpeechStartTimeForBargeIn ?? now)
    return elapsed >= bargeInHoldDuration
  }

  public func start() throws {
    guard !running else { return }
    let inputNode = engine.inputNode
    
    // âœ… è¿½åŠ : å¼·åŠ›ãªAECæœ‰åŠ¹åŒ–è¨­å®š (iOS 13+)
    // VoiceProcessingãƒ¢ãƒ¼ãƒ‰ã§ã‚ã£ã¦ã‚‚ã€æ˜ç¤ºçš„ã«ãƒã‚¤ãƒ‘ã‚¹ç„¡åŠ¹ï¼ˆï¼å‡¦ç†æœ‰åŠ¹ï¼‰ã‚’è¨­å®šã™ã‚‹ã®ãŒå®‰å…¨
    if #available(iOS 13.0, *) {
      setVoiceProcessingEnabled(true)
    }
    
    // âœ… æ—¢ã«ã‚¿ãƒƒãƒ—ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å…ˆã«å‰Šé™¤ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
    inputNode.removeTap(onBus: 0)
    
    // âœ… ã‚¨ãƒ³ã‚¸ãƒ³ã®æ‰€æœ‰æ¨©ãƒ­ã‚¸ãƒƒã‚¯
    if ownsEngine && !engine.isRunning {
        try engine.start()
        print("âœ… MicrophoneCapture: ã‚¨ãƒ³ã‚¸ãƒ³ã‚’é–‹å§‹ï¼ˆstart()å‘¼ã³å‡ºã—æ™‚ã€ownsEngine=trueï¼‰")
    } else if !ownsEngine && !engine.isRunning {
        // å…±é€šã‚¨ãƒ³ã‚¸ãƒ³ã®å ´åˆã€ã“ã“ã§startã§ããªã„ãŒã€ConversationControllerå´ã§startæ¸ˆã¿ã®ã¯ãš
        print("âš ï¸ MicrophoneCapture: ã‚¨ãƒ³ã‚¸ãƒ³ãŒåœæ­¢çŠ¶æ…‹ã§ã™")
    }
    
    // ------------------------------------------------------------
    // âœ… ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ±ºå®šãƒ­ã‚¸ãƒƒã‚¯ (ãƒ•ã‚¡ã‚¤ãƒŠãƒ«)
    // ------------------------------------------------------------
    
    var tapFormat: AVAudioFormat?
    let inputFormat = inputNode.outputFormat(forBus: 0)
    
    if inputFormat.sampleRate > 0 && inputFormat.channelCount > 0 {
        // ã‚¨ãƒ³ã‚¸ãƒ³ãŒã™ã§ã«æœ‰åŠ¹ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æŒã£ã¦ã„ã‚‹ãªã‚‰ãã‚Œã‚’ä½¿ã†
        print("âœ… MicrophoneCapture: æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä½¿ç”¨: \(inputFormat.sampleRate)Hz")
        tapFormat = inputFormat
    } else {
        // âš ï¸ 0Hzã®å ´åˆ: nilã¯ãƒ€ãƒ¡(ã‚¯ãƒ©ãƒƒã‚·ãƒ¥)ã€é©å½“ãª48kã‚‚ãƒ€ãƒ¡(ã‚¯ãƒ©ãƒƒã‚·ãƒ¥)
        // iOSã®VoiceProcessingIOå…¥åŠ›ã®ã€Œæ­£è§£ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æ‰‹å‹•æ§‹ç¯‰ã—ã¦æ¸¡ã™
        print("âš ï¸ MicrophoneCapture: 0Hzæ¤œå‡º -> 48kHz/Float32ã‚’æ‰‹å‹•æ§‹ç¯‰ã—ã¾ã™")
        
        // ã€é‡è¦ã€‘commonFormat: .pcmFormatFloat32, interleaved: false ãŒiOSãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®æ¨™æº–
        tapFormat = AVAudioFormat(
            commonFormat: .pcmFormatFloat32,
            sampleRate: 48000,
            channels: 1,
            interleaved: false
        )
    }
    
    // ä¸‡ãŒä¸€ä½œæˆå¤±æ•—ã—ãŸã‚‰å®‰å…¨ã«åœæ­¢
    guard let safeFormat = tapFormat else {
        throw NSError(domain: "MicrophoneCapture", code: -1, userInfo: [
            NSLocalizedDescriptionKey: "ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰å¤±æ•—"
        ])
    }

    // ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºè¨ˆç®—
    let framesPer20ms = AVAudioFrameCount(safeFormat.sampleRate * 0.02)
    audioBuffer.removeAll()
    lastFlushTime = Date()
    isFirstBuffer = true  // âœ… åˆå›ãƒãƒƒãƒ•ã‚¡å—ä¿¡ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
    startTime = Date()  // âœ… ãƒã‚¤ã‚¯é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²ï¼ˆåˆæœŸãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—ç”¨ï¼‰

    // ------------------------------------------------------------
    // âœ… Tapã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (æ‰‹å‹•æ§‹ç¯‰ã—ãŸ safeFormat ã‚’ä½¿ç”¨)
    // ------------------------------------------------------------
    inputNode.installTap(onBus: 0, bufferSize: framesPer20ms, format: safeFormat) { [weak self] buffer, _ in
        guard let self = self else { return }
        
        // --- Converterä½œæˆãƒ­ã‚¸ãƒƒã‚¯ ---
        if self.converter == nil || self.isFirstBuffer {
            let actualFormat = buffer.format
            if actualFormat.sampleRate == 0 { return } // ã‚¬ãƒ¼ãƒ‰
            
            if self.converter == nil {
                print("âœ… MicrophoneCapture: Converterä½œæˆ Input:\(actualFormat.sampleRate)Hz -> Output:\(self.outFormat.sampleRate)Hz")
                let conv = AVAudioConverter(from: actualFormat, to: self.outFormat)
                conv?.sampleRateConverterQuality = .max
                self.converter = conv
            }
            self.isFirstBuffer = false
        }
      
      guard let converter = self.converter else { return }
      
      // âœ… åˆå›æ¥ç¶šæ™‚ã®éŸ³å£°èªè­˜å•é¡Œå¯¾ç­–ï¼šãƒã‚¤ã‚¯é–‹å§‹ç›´å¾Œã®åˆæœŸãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚¹ã‚­ãƒƒãƒ—
      if let startTime = self.startTime {
        let elapsed = Date().timeIntervalSince(startTime) * 1000.0  // ãƒŸãƒªç§’
        if elapsed < self.initialSkipDurationMs {
          // é–‹å§‹å¾Œ200msä»¥å†…ã¯éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡ã—ãªã„ï¼ˆåˆæœŸãƒã‚¤ã‚ºã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
          return
        } else {
          // 200msçµŒéã—ãŸã‚‰ã€ä»¥é™ã¯é€šå¸¸é€šã‚Šå‡¦ç†
          self.startTime = nil  // ãƒ•ãƒ©ã‚°ã‚’ã‚¯ãƒªã‚¢ï¼ˆä¸€åº¦ã ã‘ãƒã‚§ãƒƒã‚¯ï¼‰
          print("âœ… MicrophoneCapture: åˆæœŸãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—æœŸé–“çµ‚äº†ï¼ˆ\(String(format: "%.1f", elapsed))msçµŒéï¼‰")
        }
      }
      
      // âœ… AECå¯¾ç­–ï¼šå†ç”Ÿä¸­ã‚²ãƒ¼ãƒˆåˆ¶å¾¡
      let inputRMS = self.calculateRMS(from: buffer)
      
      // -------------------------------------------------------
      // âœ… è¿½åŠ : è¨ˆç®—ã—ãŸRMSéŸ³é‡ã‚’å¤–éƒ¨ã¸é€šçŸ¥
      // -------------------------------------------------------
      self.onVolume?(inputRMS)
      if let vadSamples = self.downsampleTo16kSamples(from: buffer) {
        self.enqueueVADProcessing(samples: vadSamples)
      }
      
      let outputRMS = self.outputMonitor?.currentRMS ?? -60.0
      // æœ€æ–°ã®VADç¢ºç‡ï¼ˆSileroVADï¼‰ã‚’ãƒãƒ¼ã‚¸ã‚¤ãƒ³åˆ¤å®šã«ã‚‚ä½¿ç”¨
      let vadProbability: Float = {
        self.vadProbabilityLock.lock(); defer { self.vadProbabilityLock.unlock() }
        return self.latestVADProbability
      }()
      
      if self.isAIPlayingAudio && !self.userBargeIn {
        // ãƒãƒ¼ã‚¸ã‚¤ãƒ³åˆ¤å®š
        if self.checkBargeIn(inputRMS: inputRMS, outputRMS: outputRMS, vadProbability: vadProbability) {
          self.userBargeIn = true
          let elapsedMs = Int((Date().timeIntervalSince(self.vadSpeechStartTimeForBargeIn ?? Date())) * 1000)
        print("ğŸ¤ MicrophoneCapture: ãƒãƒ¼ã‚¸ã‚¤ãƒ³æ¤œå‡º - VAD \(String(format: "%.4f", vadProbability)) ã‚’ \(elapsedMs)ms ç¶™ç¶šæ¤œå‡º (inputRMS: \(String(format: "%.1f", inputRMS))dB, outputRMS: \(String(format: "%.1f", outputRMS))dB)")
          // âœ… ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ä¸Šä½ã¸é€šçŸ¥ï¼ˆUIã‚¹ãƒ¬ãƒƒãƒ‰ã§å‡¦ç†ï¼‰
          DispatchQueue.main.async { [weak self] in
            self?.onBargeIn?()
          }
          // ãƒãƒ¼ã‚¸ã‚¤ãƒ³æˆç«‹æ™‚ã¯é€ä¿¡ã‚’è¨±å¯ï¼ˆresponse.cancelã¯ä¸Šä½ã§é€ä¿¡ï¼‰
        } else {
          // å†ç”Ÿä¸­ã§ãƒãƒ¼ã‚¸ã‚¤ãƒ³æœªæ¤œå‡ºï¼šé€ä¿¡ã—ãªã„ï¼ˆãƒ«ãƒ¼ãƒ—æ ¹çµ¶ï¼‰
          return
        }
      }

      // âœ… é€ã‚‹å‰ã«24kHz mono PCM16LEã¸å¿…ãšãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒ«ï¼†é‡å­åŒ–
      // âœ… AVAudioEngineã®ã‚¿ãƒƒãƒ—ã¯Float32ãŒå¤šã„ã€‚å¸¸ã«AVAudioConverterã§24kHz/mono/Int16ã«å¤‰æ›
      // âœ… å¤‰æ›ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆé€ä¿¡ç”¨ï¼‰ï¼š24kHz/mono/PCM16LE
      // âœ… å…¥åŠ›ãƒãƒƒãƒ•ã‚¡ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰é€ä¿¡ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¸ã®å¤‰æ›ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œ
      let ratio = self.outFormat.sampleRate / buffer.format.sampleRate
      let framesOut = AVAudioFrameCount(Double(buffer.frameLength) * ratio + 8)  // ä½™è£•ã‚’æŒãŸã›ã‚‹
      guard let outBuf = AVAudioPCMBuffer(pcmFormat: self.outFormat, frameCapacity: framesOut) else { return }
      outBuf.frameLength = framesOut

      var error: NSError?
      let status = converter.convert(to: outBuf, error: &error) { inCount, outStatus in
        outStatus.pointee = .haveData
        return buffer
      }

      if (status == .haveData || status == .endOfStream), outBuf.frameLength > 0 {
        // âœ… ãƒãƒƒãƒ•ã‚¡ã«è“„ç©ï¼ˆ20msã”ã¨ã«è¿½åŠ ï¼‰
        // âœ… å¿…ãšint16ChannelDataã‹ã‚‰å–å¾—ï¼ˆPCM16LEå½¢å¼ï¼‰
        guard let ch0 = outBuf.int16ChannelData else { return }
        let byteCount = Int(outBuf.frameLength) * MemoryLayout<Int16>.size
        let ptr = ch0.pointee
        
        self.flushQueue.async {
          // âœ… éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ ï¼ˆPCM16LEå½¢å¼ã®Dataï¼‰
          self.audioBuffer.append(Data(bytes: ptr, count: byteCount))
          
          // âœ… 60msçµŒéã—ãŸã‚‰ãƒãƒƒãƒé€ä¿¡ï¼ˆåå¿œé€Ÿåº¦é‡è¦–ï¼‰
          let now = Date()
          let elapsed = now.timeIntervalSince(self.lastFlushTime) * 1000.0  // ãƒŸãƒªç§’
          if elapsed >= self.batchDurationMs && !self.audioBuffer.isEmpty {
            let batchData = self.audioBuffer
            self.audioBuffer.removeAll()
            self.lastFlushTime = now
            
            // âœ… ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’AVAudioPCMBufferã¨ã—ã¦å†æ§‹ç¯‰
            let batchFrames = batchData.count / MemoryLayout<Int16>.size
            guard batchFrames > 0,
                  let batchBuf = AVAudioPCMBuffer(pcmFormat: self.outFormat, frameCapacity: AVAudioFrameCount(batchFrames)) else {
              return
            }
            batchBuf.frameLength = AVAudioFrameCount(batchFrames)
            
            if let batchCh0 = batchBuf.int16ChannelData {
              batchData.withUnsafeBytes { rawPtr in
                guard let basePtr = rawPtr.baseAddress?.assumingMemoryBound(to: Int16.self) else { return }
                batchCh0.pointee.initialize(from: basePtr, count: batchFrames)
              }
              self.onPCM(batchBuf)  // â† 60msãƒãƒƒãƒã§ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            }
          }
        }
      }
    }

    // âœ… ã‚¨ãƒ³ã‚¸ãƒ³ã®æ‰€æœ‰æ¨©ãŒã‚ã‚‹å ´åˆã®ã¿é–‹å§‹ï¼ˆã¾ãŸã¯å†é–‹ï¼‰
    if ownsEngine && !engine.isRunning {
      try engine.start()
      print("âœ… MicrophoneCapture: ã‚¨ãƒ³ã‚¸ãƒ³ã‚’é–‹å§‹ï¼ˆã‚¿ãƒƒãƒ—ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã€ownsEngine=trueï¼‰")
    }
    // å…±é€šã‚¨ãƒ³ã‚¸ãƒ³ã®å ´åˆã€æ‰€æœ‰è€…ãŒé–‹å§‹ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€ã“ã“ã§ã¯é–‹å§‹ã—ãªã„
    running = true
  }

  public func stop() {
    guard running else { return }
    // âœ… é–‹å§‹æ™‚åˆ»ã‚’ãƒªã‚»ãƒƒãƒˆ
    startTime = nil
    // âœ… æ®‹ã‚Šã®ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
    flushQueue.sync {
      if !audioBuffer.isEmpty {
        let batchData = audioBuffer
        audioBuffer.removeAll()
        
        let batchFrames = batchData.count / MemoryLayout<Int16>.size
        if batchFrames > 0, let batchBuf = AVAudioPCMBuffer(pcmFormat: outFormat, frameCapacity: AVAudioFrameCount(batchFrames)) {
          batchBuf.frameLength = AVAudioFrameCount(batchFrames)
          if let batchCh0 = batchBuf.int16ChannelData {
            batchData.withUnsafeBytes { rawPtr in
              guard let basePtr = rawPtr.baseAddress?.assumingMemoryBound(to: Int16.self) else { return }
              batchCh0.pointee.initialize(from: basePtr, count: batchFrames)
            }
            onPCM(batchBuf)
          }
        }
      }
    }
    engine.inputNode.removeTap(onBus: 0)
    // âœ… ã‚¨ãƒ³ã‚¸ãƒ³ã®æ‰€æœ‰æ¨©ãŒã‚ã‚‹å ´åˆã®ã¿åœæ­¢
    if ownsEngine {
      engine.stop()
      engine.reset()  // â† å€‹åˆ¥ã‚¨ãƒ³ã‚¸ãƒ³ã®å ´åˆã€å®Œå…¨ã«ãƒªã‚»ãƒƒãƒˆ
    }
    
    // â˜… å†é–‹ã«å‚™ãˆã¦åˆæœŸåŒ–
    converter = nil
    vadConverter = nil
    userBargeIn = false
    vadSpeechStartTimeForBargeIn = nil
    recentInputRMS.removeAll()
    isFirstBuffer = true
    // âœ… enqueueVADProcessing ã¨åŒã˜ã‚­ãƒ¥ãƒ¼ã§VADçŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆä¸¦è¡Œã‚¢ã‚¯ã‚»ã‚¹ã§ã®ã‚¯ãƒ©ãƒƒã‚·ãƒ¥é˜²æ­¢ï¼‰
    resetVADState()
    
    running = false
  }
}

extension MicrophoneCapture {
  /// VADé–¢é€£ã®çŠ¶æ…‹ã‚’vadQueueä¸Šã§å®‰å…¨ã«ãƒªã‚»ãƒƒãƒˆã™ã‚‹
  private func resetVADState() {
    let work = { [weak self] in
      guard let self else { return }
      self.vadBuffer16k.removeAll(keepingCapacity: true)
      self.vad = nil
      self.vadLogCounter = 0
      self.lastVADSignalLogTime = nil
      self.vadProbabilityLock.lock()
      self.latestVADProbability = 0.0
      self.vadProbabilityLock.unlock()
    }
    if DispatchQueue.getSpecific(key: vadQueueSpecificKey) == true {
      work()
    } else {
      vadQueue.sync(execute: work)
    }
  }

  /// VoiceProcessingã®ON/OFFã‚’å®‰å…¨ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ï¼ˆiOS 13+ã®ã¿ï¼‰
  private func setVoiceProcessingEnabled(_ enabled: Bool) {
    guard #available(iOS 13.0, *) else { return }
    do {
      try engine.inputNode.setVoiceProcessingEnabled(enabled)
      print("âœ… MicrophoneCapture: VoiceProcessingEnabled = \(enabled)")
    } catch {
      print("âš ï¸ MicrophoneCapture: VoiceProcessingEnabledè¨­å®šå¤±æ•— - \(error)")
    }
  }

  // âœ… 48kHzâ†’16kHzã¸é–“å¼•ãã—ã¦VADã«æ¸¡ã™
  private func downsampleTo16kSamples(from buffer: AVAudioPCMBuffer) -> [Float]? {
    let inputRate = buffer.format.sampleRate
    let ratio = inputRate / vadTargetSampleRate

    // Fast path: exact 48k->16k decimation by 3 with stride
    if abs(ratio - 3.0) < 0.01 {
      let step = 3
      let frameCount = Int(buffer.frameLength)
      var samples: [Float] = []
      samples.reserveCapacity(frameCount / step + 1)

      if let floatChannel = buffer.floatChannelData?.pointee {
        for i in stride(from: 0, to: frameCount, by: step) {
          samples.append(floatChannel[i])
        }
        return samples
      } else if let int16Channel = buffer.int16ChannelData?.pointee {
        let scale = Float(Int16.max)
        for i in stride(from: 0, to: frameCount, by: step) {
          samples.append(Float(int16Channel[i]) / scale)
        }
        return samples
      }
      return nil
    }

    // General path: use AVAudioConverter for non-48k inputs (e.g., 44.1k or 24k)
    let targetFormat = AVAudioFormat(commonFormat: .pcmFormatFloat32,
                                     sampleRate: vadTargetSampleRate,
                                     channels: buffer.format.channelCount,
                                     interleaved: false)
    if vadConverter == nil || vadConverter?.inputFormat.sampleRate != inputRate {
      vadConverter = AVAudioConverter(from: buffer.format, to: targetFormat!)
      vadConverter?.sampleRateConverterQuality = .max
    }
    guard let converter = vadConverter,
          let targetFormat else { return nil }

    let inFrames = Int(buffer.frameLength)
    let outFrames = Int(Double(inFrames) * (vadTargetSampleRate / inputRate) + 8)
    guard let outBuf = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: AVAudioFrameCount(outFrames)) else {
      return nil
    }

    var error: NSError?
    let status = converter.convert(to: outBuf, error: &error) { _, outStatus in
      outStatus.pointee = .haveData
      return buffer
    }
    guard (status == .haveData || status == .endOfStream),
          let floatChannel = outBuf.floatChannelData?.pointee else {
      if let error { print("âŒ VAD converter error: \(error)") }
      return nil
    }

    let framesOut = Int(outBuf.frameLength)
    return Array(UnsafeBufferPointer(start: floatChannel, count: framesOut))
  }

  private func enqueueVADProcessing(samples: [Float]) {
    guard !samples.isEmpty else { return }
    vadQueue.async { [weak self] in
      guard let self else { return }
      if self.vad == nil {
        self.vad = try? SileroVAD()
        if self.vad == nil {
          print("âŒ MicrophoneCapture: SileroVAD init failed")
        } else {
          print("ğŸ¯ MicrophoneCapture: SileroVAD initialized")
        }
      }
      guard let vad = self.vad else { return }

      self.vadBuffer16k.append(contentsOf: samples)
      while self.vadBuffer16k.count >= self.vadChunkSize {
        // âœ… å¿µã®ãŸã‚å†ç¢ºèªï¼ˆstop()ç­‰ã§ãƒªã‚»ãƒƒãƒˆãŒèµ°ã£ãŸå ´åˆã§ã‚‚ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ãªã„ï¼‰
        guard self.vadBuffer16k.count >= self.vadChunkSize else { break }
        let chunk = Array(self.vadBuffer16k.prefix(self.vadChunkSize))
        self.vadBuffer16k.removeFirst(self.vadChunkSize)

        // âœ… å…¥åŠ›ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼šVADã®è¦³æ¸¬/åˆ¤å®šç”¨ã«ã€Œå¢—å¹…ã—ãªã„ã€æ–¹é‡ï¼ˆãƒªãƒŸãƒƒã‚¿ã®ã¿ï¼‰
        // - gainã‚’ç„¡éŸ³ã§æŒã¡ä¸Šã’ã‚‹ã¨ãƒã‚¤ã‚ºãŒå¢—ãˆã€RMSåˆ†æã‚‚å£Šã‚Œã‚‹
        // - ã‚¯ãƒªãƒƒãƒ—ã ã‘ã‚’é¿ã‘ã‚‹ï¼ˆpeak>0.98 ã®ã¨ãã®ã¿æ¸›è¡°ï¼‰
        let rms = sqrt(chunk.reduce(0) { $0 + $1 * $1 } / Float(chunk.count))
        let peak = chunk.reduce(0) { max($0, abs($1)) }

        let limiterCeil: Float = 0.98
        var gain: Float = 1.0
        if peak > 1e-6, peak > limiterCeil {
          gain = limiterCeil / peak
        }

        var clippedCount = 0
        let scaled = chunk.map { sample -> Float in
          let v = sample * gain
          if v > 1 { clippedCount += 1; return 1 }
          if v < -1 { clippedCount += 1; return -1 }
          return v
        }

        if shouldLogVADSignal() {
          let nonZeroCount = chunk.reduce(0) { $0 + (abs($1) > 1e-6 ? 1 : 0) }
          let nonZeroRatio = Double(nonZeroCount) / Double(chunk.count)
          let rmsDb = rms > 1e-9 ? 20 * log10(Double(rms)) : -120.0
          let clipRatio = Double(clippedCount) / Double(chunk.count)
          print("ğŸ¯ MicrophoneCapture: VAD input stats - peak=\(String(format: "%.6f", peak)), rms=\(String(format: "%.2f", rmsDb))dB, nonZero=\(String(format: "%.2f", nonZeroRatio)), samples=\(chunk.count), sr=16k | scaled(gain=\(String(format: "%.3f", gain)), clip=\(String(format: "%.3f", clipRatio)))")
        }

        let probability = vad.process(segment: scaled)
        // VADæœ€æ–°å€¤ã‚’ä¿æŒï¼ˆãƒãƒ¼ã‚¸ã‚¤ãƒ³åˆ¤å®šã¨å…±æœ‰ï¼‰
        self.vadProbabilityLock.lock()
        self.latestVADProbability = probability
        self.vadProbabilityLock.unlock()
        self.vadLogCounter &+= 1
        // ãƒ‡ãƒãƒƒã‚°ç”¨: VADç¢ºç‡ã¨å…¥åŠ›RMSã‚’é©åº¦ãªé »åº¦ã§ãƒ­ã‚°å‡ºåŠ›ï¼ˆç´„640msã”ã¨ï¼‰
        if self.vadLogCounter % 20 == 0 {
          print("ğŸ¯ MicrophoneCapture: VAD prob=\(String(format: "%.4f", probability)), rms=\(String(format: "%.6f", rms)), gain=\(String(format: "%.3f", gain))")
        }
        if let callback = self.onVADProbability {
          DispatchQueue.main.async {
            callback(probability)
          }
        } else {
          print("âš ï¸ MicrophoneCapture: onVADProbability not set (prob=\(String(format: "%.4f", probability)))")
        }
      }
    }
  }

  private func shouldLogVADSignal() -> Bool {
    let now = Date()
    if let last = lastVADSignalLogTime, now.timeIntervalSince(last) < 1.0 {
      return false
    }
    lastVADSignalLogTime = now
    return true
  }
}
